{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Memory: Building Memory-Enabled Agents with LangGraph\n",
    "\n",
    "In this notebook, we'll explore **agent memory systems** - the ability for AI agents to remember information across interactions. We'll implement all five memory types from the **CoALA (Cognitive Architectures for Language Agents)** framework while building on our Personal Wellness Assistant use case.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the 5 memory types from the CoALA framework\n",
    "- Implement short-term memory with checkpointers and thread_id\n",
    "- Build long-term memory with InMemoryStore and namespaces\n",
    "- Use semantic memory for meaning-based retrieval\n",
    "- Apply episodic memory for few-shot learning from past experiences\n",
    "- Create procedural memory for self-improving agents\n",
    "- Combine all memory types into a unified wellness agent\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "- **Breakout Room #1:** Memory Foundations\n",
    "  - Task 1: Dependencies\n",
    "  - Task 2: Understanding Agent Memory (CoALA Framework)\n",
    "  - Task 3: Short-Term Memory (MemorySaver, thread_id)\n",
    "  - Task 4: Long-Term Memory (InMemoryStore, namespaces)\n",
    "  - Task 5: Message Trimming & Context Management\n",
    "  - Question #1 & Question #2\n",
    "  - üèóÔ∏è Activity #1: Store & Retrieve User Wellness Profile\n",
    "\n",
    "- **Breakout Room #2:** Advanced Memory & Integration\n",
    "  - Task 6: Semantic Memory (Embeddings + Search)\n",
    "  - Task 7: Building Semantic Wellness Knowledge Base\n",
    "  - Task 8: Episodic Memory (Few-Shot Learning)\n",
    "  - Task 9: Procedural Memory (Self-Improving Agent)\n",
    "  - Task 10: Unified Wellness Memory Agent\n",
    "  - Question #3 & Question #4\n",
    "  - üèóÔ∏è Activity #2: Wellness Memory Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ü§ù Breakout Room #1\n",
    "## Memory Foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Dependencies\n",
    "\n",
    "Before we begin, make sure you have:\n",
    "\n",
    "1. **API Keys** for:\n",
    "   - OpenAI (for GPT-4o-mini and embeddings)\n",
    "   - LangSmith (optional, for tracing)\n",
    "\n",
    "2. **Dependencies installed** via `uv sync`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import getpass\n",
    "from uuid import uuid4\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Required for async operations in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API Keys\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith tracing enabled. Project: AIE9 - Agent Memory - 4c3e77a2\n"
     ]
    }
   ],
   "source": [
    "# Optional: LangSmith for tracing\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE9 - Agent Memory - {uuid4().hex[0:8]}\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key (press Enter to skip): \") or \"\"\n",
    "\n",
    "# if not os.environ[\"LANGCHAIN_API_KEY\"]:\n",
    "#     os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "#     print(\"LangSmith tracing disabled\")\n",
    "# else:\n",
    "#     print(f\"LangSmith tracing enabled. Project: {os.environ['LANGCHAIN_PROJECT']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory systems ready!\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Test the connection\n",
    "response = llm.invoke(\"Say 'Memory systems ready!' in exactly those words.\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Understanding Agent Memory (CoALA Framework)\n",
    "\n",
    "The **CoALA (Cognitive Architectures for Language Agents)** framework identifies 5 types of memory that agents can use:\n",
    "\n",
    "| Memory Type | Human Analogy | AI Implementation | Wellness Example |\n",
    "|-------------|---------------|-------------------|------------------|\n",
    "| **Short-term** | What someone just said | Conversation history within a thread | Current consultation conversation |\n",
    "| **Long-term** | Remembering a friend's birthday | User preferences stored across sessions | User's goals, allergies, conditions |\n",
    "| **Semantic** | Knowing Paris is in France | Facts retrieved by meaning | Wellness knowledge retrieval |\n",
    "| **Episodic** | Remembering your first day at work | Learning from past experiences | Past successful advice patterns |\n",
    "| **Procedural** | Knowing how to ride a bike | Self-improving instructions | Learned communication preferences |\n",
    "\n",
    "### Memory Architecture Overview\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    LangGraph Wellness Agent                     ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ\n",
    "‚îÇ  ‚îÇ  Short-term  ‚îÇ  ‚îÇ  Long-term   ‚îÇ  ‚îÇ   Semantic   ‚îÇ           ‚îÇ\n",
    "‚îÇ  ‚îÇ    Memory    ‚îÇ  ‚îÇ    Memory    ‚îÇ  ‚îÇ    Memory    ‚îÇ           ‚îÇ\n",
    "‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ           ‚îÇ\n",
    "‚îÇ  ‚îÇ Checkpointer ‚îÇ  ‚îÇ    Store     ‚îÇ  ‚îÇStore+Embed   ‚îÇ           ‚îÇ\n",
    "‚îÇ  ‚îÇ + thread_id  ‚îÇ  ‚îÇ + namespace  ‚îÇ  ‚îÇ  + search()  ‚îÇ           ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                             ‚îÇ\n",
    "‚îÇ  ‚îÇ   Episodic   ‚îÇ  ‚îÇ  Procedural  ‚îÇ                             ‚îÇ\n",
    "‚îÇ  ‚îÇ    Memory    ‚îÇ  ‚îÇ    Memory    ‚îÇ                             ‚îÇ\n",
    "‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ                             ‚îÇ\n",
    "‚îÇ  ‚îÇ  Few-shot    ‚îÇ  ‚îÇSelf-modifying‚îÇ                             ‚îÇ\n",
    "‚îÇ  ‚îÇ  examples    ‚îÇ  ‚îÇ   prompts    ‚îÇ                             ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                             ‚îÇ\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Key LangGraph Components\n",
    "\n",
    "| Component | Memory Type | Scope |\n",
    "|-----------|-------------|-------|\n",
    "| `MemorySaver` (Checkpointer) | Short-term | Within a single thread |\n",
    "| `InMemoryStore` | Long-term, Semantic, Episodic, Procedural | Across all threads |\n",
    "| `thread_id` | Short-term | Identifies unique conversations |\n",
    "| Namespaces | All store-based | Organizes memories by user/purpose |\n",
    "\n",
    "**Documentation:**\n",
    "- [CoALA Paper](https://arxiv.org/abs/2309.02427)\n",
    "- [LangGraph Memory Concepts](https://langchain-ai.github.io/langgraph/concepts/memory/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Short-Term Memory (MemorySaver, thread_id)\n",
    "\n",
    "**Short-term memory** maintains context within a single conversation thread. Think of it like your working memory during a phone call - you remember what was said earlier, but once the call ends, those details fade.\n",
    "\n",
    "In LangGraph, short-term memory is implemented through:\n",
    "- **Checkpointer**: Saves the graph state at each step\n",
    "- **thread_id**: Uniquely identifies each conversation\n",
    "\n",
    "### How It Works\n",
    "\n",
    "```\n",
    "Thread 1: \"Hi, I'm Alice\"          Thread 2: \"What's my name?\"\n",
    "     ‚îÇ                                   ‚îÇ\n",
    "     ‚ñº                                   ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Checkpointer ‚îÇ                   ‚îÇ Checkpointer ‚îÇ\n",
    "‚îÇ  thread_1    ‚îÇ                   ‚îÇ  thread_2    ‚îÇ\n",
    "‚îÇ              ‚îÇ                   ‚îÇ              ‚îÇ\n",
    "‚îÇ [\"Hi Alice\"] ‚îÇ                   ‚îÇ [empty]      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "     ‚îÇ                                   ‚îÇ\n",
    "     ‚ñº                                   ‚ñº\n",
    "\"Hi Alice!\"                        \"I don't know your name\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wellness chatbot compiled with short-term memory (checkpointing)\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# Define the state schema for our graph\n",
    "# The `add_messages` annotation tells LangGraph how to update the messages list\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# Define our wellness chatbot node\n",
    "def wellness_chatbot(state: State):\n",
    "    \"\"\"Process the conversation and generate a wellness-focused response.\"\"\"\n",
    "    system_prompt = SystemMessage(content=\"\"\"You are a friendly Personal Wellness Assistant. \n",
    "Help users with exercise, nutrition, sleep, and stress management questions.\n",
    "Be supportive and remember details the user shares about themselves.\"\"\")\n",
    "    \n",
    "    messages = [system_prompt] + state[\"messages\"]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"chatbot\", wellness_chatbot)\n",
    "builder.add_edge(START, \"chatbot\")\n",
    "builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "# Compile with a checkpointer for short-term memory\n",
    "checkpointer = MemorySaver()\n",
    "wellness_graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "print(\"Wellness chatbot compiled with short-term memory (checkpointing)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hi! My name is Sarah and I want to improve my sleep.\n",
      "Assistant: Hi Sarah! It's great to meet you! Improving sleep is such an important goal for overall wellness. Can you share a bit about your current sleep routine or any specific challenges you're facing with sleep? This will help me provide you with tailored advice!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test short-term memory within a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"wellness_thread_1\"}}\n",
    "\n",
    "# First message - introduce ourselves\n",
    "response = wellness_graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Hi! My name is Sarah and I want to improve my sleep.\")]},\n",
    "    config\n",
    ")\n",
    "print(\"User: Hi! My name is Sarah and I want to improve my sleep.\")\n",
    "print(f\"Assistant: {response['messages'][-1].content}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What's my name and what am I trying to improve?\n",
      "Assistant: Your name is Sarah, and you're looking to improve your sleep. How can I assist you further with that?\n"
     ]
    }
   ],
   "source": [
    "# Second message - test if it remembers (same thread)\n",
    "response = wellness_graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What's my name and what am I trying to improve?\")]},\n",
    "    config  # Same config = same thread_id\n",
    ")\n",
    "print(\"User: What's my name and what am I trying to improve?\")\n",
    "print(f\"Assistant: {response['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User (NEW thread): What's my name?\n",
      "Assistant: I don't have your name yet! If you'd like to share it, I can remember it for our future conversations. How can I assist you today?\n",
      "\n",
      "Notice: The agent doesn't know our name because this is a new thread!\n"
     ]
    }
   ],
   "source": [
    "# New thread - it won't remember Sarah!\n",
    "different_config = {\"configurable\": {\"thread_id\": \"wellness_thread_2\"}}\n",
    "\n",
    "response = wellness_graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What's my name?\")]},\n",
    "    different_config  # Different thread_id = no memory of Sarah\n",
    ")\n",
    "print(\"User (NEW thread): What's my name?\")\n",
    "print(f\"Assistant: {response['messages'][-1].content}\")\n",
    "print()\n",
    "print(\"Notice: The agent doesn't know our name because this is a new thread!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 1 has 4 messages:\n",
      "  User: Hi! My name is Sarah and I want to improve my sleep.\n",
      "  Assistant: Hi Sarah! It's great to meet you! Improving sleep is such an important goal for ...\n",
      "  User: What's my name and what am I trying to improve?\n",
      "  Assistant: Your name is Sarah, and you're looking to improve your sleep. How can I assist y...\n"
     ]
    }
   ],
   "source": [
    "# Inspect the state of thread 1\n",
    "state = wellness_graph.get_state(config)\n",
    "print(f\"Thread 1 has {len(state.values['messages'])} messages:\")\n",
    "for msg in state.values['messages']:\n",
    "    role = \"User\" if isinstance(msg, HumanMessage) else \"Assistant\"\n",
    "    content = msg.content[:80] + \"...\" if len(msg.content) > 80 else msg.content\n",
    "    print(f\"  {role}: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Long-Term Memory (InMemoryStore, namespaces)\n",
    "\n",
    "**Long-term memory** stores information across different conversation threads. This is like remembering that your friend prefers tea over coffee - you remember it every time you meet them, regardless of what you're currently discussing.\n",
    "\n",
    "In LangGraph, long-term memory uses:\n",
    "- **Store**: A persistent key-value store\n",
    "- **Namespaces**: Organize memories by user, application, or context\n",
    "\n",
    "### Key Difference from Short-Term Memory\n",
    "\n",
    "| Short-Term (Checkpointer) | Long-Term (Store) |\n",
    "|---------------------------|-------------------|\n",
    "| Scoped to a single thread | Shared across all threads |\n",
    "| Automatic (messages) | Explicit (you decide what to store) |\n",
    "| Conversation history | User preferences, facts, etc. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored Sarah's profile and preferences in long-term memory\n"
     ]
    }
   ],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# Create a store for long-term memory\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Namespaces organize memories - typically by user_id and category\n",
    "user_id = \"user_sarah\"\n",
    "profile_namespace = (user_id, \"profile\")\n",
    "preferences_namespace = (user_id, \"preferences\")\n",
    "\n",
    "# Store Sarah's wellness profile\n",
    "store.put(profile_namespace, \"name\", {\"value\": \"Sarah\"})\n",
    "store.put(profile_namespace, \"goals\", {\"primary\": \"improve sleep\", \"secondary\": \"reduce stress\"})\n",
    "store.put(profile_namespace, \"conditions\", {\"allergies\": [\"peanuts\"], \"injuries\": [\"bad knee\"]})\n",
    "\n",
    "# Store Sarah's preferences\n",
    "store.put(preferences_namespace, \"communication\", {\"style\": \"friendly\", \"detail_level\": \"moderate\"})\n",
    "store.put(preferences_namespace, \"schedule\", {\"preferred_workout_time\": \"morning\", \"available_days\": [\"Mon\", \"Wed\", \"Fri\"]})\n",
    "\n",
    "print(\"Stored Sarah's profile and preferences in long-term memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: {'value': 'Sarah'}\n",
      "Goals: {'primary': 'improve sleep', 'secondary': 'reduce stress'}\n",
      "\n",
      "All profile items:\n",
      "  name: {'value': 'Sarah'}\n",
      "  goals: {'primary': 'improve sleep', 'secondary': 'reduce stress'}\n",
      "  conditions: {'allergies': ['peanuts'], 'injuries': ['bad knee']}\n"
     ]
    }
   ],
   "source": [
    "# Retrieve specific memories\n",
    "name = store.get(profile_namespace, \"name\")\n",
    "print(f\"Name: {name.value}\")\n",
    "\n",
    "goals = store.get(profile_namespace, \"goals\")\n",
    "print(f\"Goals: {goals.value}\")\n",
    "\n",
    "# List all memories in a namespace\n",
    "print(\"\\nAll profile items:\")\n",
    "for item in store.search(profile_namespace):\n",
    "    print(f\"  {item.key}: {item.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personalized graph compiled with both short-term and long-term memory\n"
     ]
    }
   ],
   "source": [
    "from langgraph.store.base import BaseStore\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# Define state with user_id for personalization\n",
    "class PersonalizedState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_id: str\n",
    "\n",
    "\n",
    "def personalized_wellness_chatbot(state: PersonalizedState, config: RunnableConfig, *, store: BaseStore):\n",
    "    \"\"\"A wellness chatbot that uses long-term memory for personalization.\"\"\"\n",
    "    user_id = state[\"user_id\"]\n",
    "    profile_namespace = (user_id, \"profile\")\n",
    "    preferences_namespace = (user_id, \"preferences\")\n",
    "    \n",
    "    # Retrieve user profile from long-term memory\n",
    "    profile_items = list(store.search(profile_namespace))\n",
    "    pref_items = list(store.search(preferences_namespace))\n",
    "    \n",
    "    # Build context from profile\n",
    "    profile_text = \"\\n\".join([f\"- {p.key}: {p.value}\" for p in profile_items])\n",
    "    pref_text = \"\\n\".join([f\"- {p.key}: {p.value}\" for p in pref_items])\n",
    "    \n",
    "    system_msg = f\"\"\"You are a Personal Wellness Assistant. You know the following about this user:\n",
    "\n",
    "PROFILE:\n",
    "{profile_text if profile_text else 'No profile stored.'}\n",
    "\n",
    "PREFERENCES:\n",
    "{pref_text if pref_text else 'No preferences stored.'}\n",
    "\n",
    "Use this information to personalize your responses. Be supportive and helpful.\"\"\"\n",
    "    \n",
    "    messages = [SystemMessage(content=system_msg)] + state[\"messages\"]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Build the personalized graph\n",
    "builder2 = StateGraph(PersonalizedState)\n",
    "builder2.add_node(\"chatbot\", personalized_wellness_chatbot)\n",
    "builder2.add_edge(START, \"chatbot\")\n",
    "builder2.add_edge(\"chatbot\", END)\n",
    "\n",
    "# Compile with BOTH checkpointer (short-term) AND store (long-term)\n",
    "personalized_graph = builder2.compile(\n",
    "    checkpointer=MemorySaver(),\n",
    "    store=store\n",
    ")\n",
    "\n",
    "print(\"Personalized graph compiled with both short-term and long-term memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What exercises would you recommend for me?\n",
      "Assistant: Hi Sarah! It's great that you're looking to improve your sleep and reduce stress through exercise. Given your bad knee, we want to focus on low-impact activities that are gentle on your joints. Here are some exercises you might enjoy, especially in the morning:\n",
      "\n",
      "1. **Walking**: A brisk walk is a fantastic way to start your day. It‚Äôs low-impact and can help clear your mind.\n",
      "\n",
      "2. **Swimming**: If you have access to a pool, swimming is excellent for a full-body workout without stressing your knee.\n",
      "\n",
      "3. **Cycling**: Riding a stationary bike or cycling outdoors can be a great way to get your heart rate up while being easy on your knees.\n",
      "\n",
      "4. **Yoga**: Gentle yoga can help with flexibility, relaxation, and stress reduction. Look for classes that focus on restorative or yin yoga.\n",
      "\n",
      "5. **Pilates**: This can strengthen your core and improve your overall body awareness without putting too much strain on your knee.\n",
      "\n",
      "6. **Strength Training**: Focus on upper body and core exercises, using resistance bands or light weights, while avoiding exercises that put pressure on your knee.\n",
      "\n",
      "Make sure to listen to your body and modify any exercises that cause discomfort. It might also be helpful to consult with a fitness professional who can tailor a program specifically for you. Let me know if you‚Äôd like more details on any of these options!\n",
      "\n",
      "Notice: The agent knows about Sarah's bad knee without her mentioning it!\n"
     ]
    }
   ],
   "source": [
    "# Test the personalized chatbot - it knows Sarah's profile!\n",
    "config = {\"configurable\": {\"thread_id\": \"personalized_thread_1\"}}\n",
    "\n",
    "response = personalized_graph.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"What exercises would you recommend for me?\")],\n",
    "        \"user_id\": \"user_sarah\"\n",
    "    },\n",
    "    config\n",
    ")\n",
    "\n",
    "print(\"User: What exercises would you recommend for me?\")\n",
    "print(f\"Assistant: {response['messages'][-1].content}\")\n",
    "print()\n",
    "print(\"Notice: The agent knows about Sarah's bad knee without her mentioning it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User (NEW thread): Can you suggest a snack for me?\n",
      "Assistant: Of course, Sarah! Here are a couple of snack ideas that are both delicious and safe for you:\n",
      "\n",
      "1. **Rice Cakes with Hummus**: Rice cakes are light and crunchy, and you can spread some hummus on top for a tasty and satisfying snack. You can also add cucumber slices for extra crunch!\n",
      "\n",
      "2. **Veggie Sticks with Guacamole**: Cut up some carrots, celery, and bell peppers, and dip them in guacamole. It‚Äôs a refreshing and nutritious option.\n",
      "\n",
      "3. **Cottage Cheese with Pineapple**: This combination is not only tasty but also provides a good source of protein and vitamins.\n",
      "\n",
      "4. **Trail Mix (without peanuts)**: You can make your own trail mix with nuts (like almonds or cashews), seeds, and dried fruit. Just be sure to avoid any mixes that contain peanuts.\n",
      "\n",
      "Let me know if you‚Äôd like more ideas or if you have any specific preferences!\n",
      "\n",
      "Notice: Even in a new thread, the agent knows Sarah has a peanut allergy!\n"
     ]
    }
   ],
   "source": [
    "# Even in a NEW thread, it still knows Sarah's profile\n",
    "# because long-term memory is cross-thread!\n",
    "\n",
    "new_config = {\"configurable\": {\"thread_id\": \"personalized_thread_2\"}}\n",
    "\n",
    "response = personalized_graph.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Can you suggest a snack for me?\")],\n",
    "        \"user_id\": \"user_sarah\"\n",
    "    },\n",
    "    new_config\n",
    ")\n",
    "\n",
    "print(\"User (NEW thread): Can you suggest a snack for me?\")\n",
    "print(f\"Assistant: {response['messages'][-1].content}\")\n",
    "print()\n",
    "print(\"Notice: Even in a new thread, the agent knows Sarah has a peanut allergy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Message Trimming & Context Management\n",
    "\n",
    "Long conversations can exceed the LLM's context window. LangGraph provides utilities to manage message history:\n",
    "\n",
    "- **`trim_messages`**: Keeps only recent messages up to a token limit\n",
    "- **Summarization**: Compress older messages into summaries\n",
    "\n",
    "### Why Trim Even with 128K Context?\n",
    "\n",
    "Even with large context windows:\n",
    "1. **Cost**: More tokens = higher API costs\n",
    "2. **Latency**: Larger contexts take longer to process\n",
    "3. **Quality**: Models can struggle with \"lost in the middle\" - important info buried in long contexts\n",
    "4. **Relevance**: Old messages may not be relevant to current query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 10 messages\n",
      "Trimmed: 10 messages\n",
      "\n",
      "Trimmed conversation:\n",
      "  System: You are a wellness assistant.\n",
      "  Human: I want to improve my health.\n",
      "  AI: Great goal! Let's start with exercise. What's your current a...\n",
      "  Human: I walk about 30 minutes a day.\n",
      "  AI: That's a good foundation. For cardiovascular health, aim for...\n",
      "  Human: What about nutrition?\n",
      "  AI: Focus on whole foods: vegetables, lean proteins, whole grain...\n",
      "  Human: And sleep?\n",
      "  AI: Aim for 7-9 hours. Maintain a consistent sleep schedule and ...\n",
      "  Human: What's the most important change I should make first?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import trim_messages\n",
    "\n",
    "# Create a trimmer that keeps only recent messages\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=500,  # Keep messages up to this token count\n",
    "    strategy=\"last\",  # Keep the most recent messages\n",
    "    token_counter=llm,  # Use the LLM to count tokens\n",
    "    include_system=True,  # Always keep system messages\n",
    "    allow_partial=False,  # Don't cut messages in half\n",
    ")\n",
    "\n",
    "# Example: Create a long conversation\n",
    "long_conversation = [\n",
    "    SystemMessage(content=\"You are a wellness assistant.\"),\n",
    "    HumanMessage(content=\"I want to improve my health.\"),\n",
    "    AIMessage(content=\"Great goal! Let's start with exercise. What's your current activity level?\"),\n",
    "    HumanMessage(content=\"I walk about 30 minutes a day.\"),\n",
    "    AIMessage(content=\"That's a good foundation. For cardiovascular health, aim for 150 minutes of moderate activity per week.\"),\n",
    "    HumanMessage(content=\"What about nutrition?\"),\n",
    "    AIMessage(content=\"Focus on whole foods: vegetables, lean proteins, whole grains. Limit processed foods and added sugars.\"),\n",
    "    HumanMessage(content=\"And sleep?\"),\n",
    "    AIMessage(content=\"Aim for 7-9 hours. Maintain a consistent sleep schedule and create a relaxing bedtime routine.\"),\n",
    "    HumanMessage(content=\"What's the most important change I should make first?\"),\n",
    "]\n",
    "\n",
    "# Trim to fit context window\n",
    "trimmed = trimmer.invoke(long_conversation)\n",
    "print(f\"Original: {len(long_conversation)} messages\")\n",
    "print(f\"Trimmed: {len(trimmed)} messages\")\n",
    "print(\"\\nTrimmed conversation:\")\n",
    "for msg in trimmed:\n",
    "    role = type(msg).__name__.replace(\"Message\", \"\")\n",
    "    content = msg.content[:60] + \"...\" if len(msg.content) > 60 else msg.content\n",
    "    print(f\"  {role}: {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized: 5 messages\n",
      "\n",
      "Summarized conversation:\n",
      "  System: You are a wellness assistant.\n",
      "  System: [Previous conversation summary: The conversation centers around the user's desir...\n",
      "  Human: And sleep?\n",
      "  AI: Aim for 7-9 hours. Maintain a consistent sleep schedule and create a relaxing be...\n",
      "  Human: What's the most important change I should make first?\n"
     ]
    }
   ],
   "source": [
    "# Summarization approach for longer conversations\n",
    "\n",
    "def summarize_conversation(messages: list, max_messages: int = 6) -> list:\n",
    "    \"\"\"Summarize older messages to manage context length.\"\"\"\n",
    "    if len(messages) <= max_messages:\n",
    "        return messages\n",
    "    \n",
    "    # Keep the system message and last few messages\n",
    "    system_msg = messages[0] if isinstance(messages[0], SystemMessage) else None\n",
    "    content_messages = messages[1:] if system_msg else messages\n",
    "    \n",
    "    if len(content_messages) <= max_messages:\n",
    "        return messages\n",
    "    \n",
    "    old_messages = content_messages[:-max_messages+1]\n",
    "    recent_messages = content_messages[-max_messages+1:]\n",
    "    \n",
    "    # Summarize old messages\n",
    "    summary_prompt = f\"\"\"Summarize this conversation in 2-3 sentences, \n",
    "capturing key wellness topics discussed and any important user information:\n",
    "\n",
    "{chr(10).join([f'{type(m).__name__}: {m.content[:200]}' for m in old_messages])}\"\"\"\n",
    "    \n",
    "    summary = llm.invoke(summary_prompt)\n",
    "    \n",
    "    # Return: system + summary + recent messages\n",
    "    result = []\n",
    "    if system_msg:\n",
    "        result.append(system_msg)\n",
    "    result.append(SystemMessage(content=f\"[Previous conversation summary: {summary.content}]\"))\n",
    "    result.extend(recent_messages)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Test summarization\n",
    "summarized = summarize_conversation(long_conversation, max_messages=4)\n",
    "print(f\"Summarized: {len(summarized)} messages\")\n",
    "print(\"\\nSummarized conversation:\")\n",
    "for msg in summarized:\n",
    "    role = type(msg).__name__.replace(\"Message\", \"\")\n",
    "    content = msg.content[:80] + \"...\" if len(msg.content) > 80 else msg.content\n",
    "    print(f\"  {role}: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚ùì Question #1:\n",
    "\n",
    "What are the trade-offs between **short-term memory** (checkpointer) vs **long-term memory** (store)? When should wellness data move from short-term to long-term?\n",
    "\n",
    "Consider:\n",
    "- What information should persist across sessions?\n",
    "- What are the privacy implications of each?\n",
    "- How would you decide what to promote from short-term to long-term?\n",
    "\n",
    "##### Answer:\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùì Question #2:\n",
    "\n",
    "Why use message trimming with a 128K context window when HealthWellnessGuide.txt is only ~16KB? What should **always** be preserved when trimming a wellness consultation?\n",
    "\n",
    "Consider:\n",
    "- The \"lost in the middle\" phenomenon\n",
    "- Cost and latency implications\n",
    "- What user information is critical for safety (allergies, conditions, etc.)\n",
    "\n",
    "##### Answer:\n",
    "Its meant to be a 2 way conversation, so user experience is impacted by response latency and larger context windows will impact that negatively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üèóÔ∏è Activity #1: Store & Retrieve User Wellness Profile\n",
    "\n",
    "Build a complete wellness profile system that:\n",
    "1. Defines a wellness profile schema (name, goals, conditions, preferences)\n",
    "2. Creates functions to store and retrieve profile data\n",
    "3. Builds a personalized wellness agent that uses the profile\n",
    "4. Tests that different users get different advice\n",
    "\n",
    "### Requirements:\n",
    "- Define at least 5 profile attributes\n",
    "- Support multiple users with different profiles\n",
    "- Agent should reference profile data in responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Step 1: Define a wellness profile schema\n",
    "# Example attributes: name, age, goals, conditions, allergies, fitness_level, preferred_activities\n",
    "\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from trustcall import create_extractor\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "class MemoryState(MessagesState):\n",
    "    user_id: str\n",
    "\n",
    "\n",
    "# Create a store for long-term memory\n",
    "store = InMemoryStore()\n",
    "\n",
    "class ProfileSchema(BaseModel):\n",
    "    name: Optional[str] = Field(description=\"name of user\", frozen=True)\n",
    "    age: Optional[str] = Field(description=\"age of user\", frozen=True)\n",
    "    gender: Optional[str] = Field(description=\"gender of user\", frozen=True)\n",
    "    occupation: Optional[str] = Field(description=\"job description indicates whether physical, or office work\", frozen=True)\n",
    "    conditions: Optional[List[str]] = Field(description=\"list of any medical conditions they may have\")\n",
    "    fitness_level: Optional[str] =  Field(description=\"self reported levels of general fitness\")\n",
    "    goals: Optional[List[str]] = Field(description=\"list of wellness related goals\")\n",
    "    sports: Optional[List[str]] =  Field(description=\"activites or sports user likes to do\")\n",
    "    available_times: Optional[List[str]] = Field(description=\"dates or times user is available\")\n",
    "\n",
    "\n",
    "\n",
    "# attributes heath_conditions, age, gender, occupation, fitness_level, medications\n",
    "\n",
    "\n",
    "SYSTEM_PROMT = \"\"\"\n",
    "    You are a personalised wellness assistant that dispenses advice to users \n",
    "    to make helpful suggestions on how to improve their physical and mental heath.\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT= \"\"\"\n",
    "    You have access to the following information about the user: \n",
    "    User profile - describes their demographics which do not change\n",
    "    and should be considered immutable.\n",
    "    Preferences - a list of current preferences but may change depending on their responses to your questions\n",
    "        <user profile> \n",
    "        {user_profile}\n",
    "        </user profile>\n",
    "\n",
    "\n",
    "    Please make recommendations with reference to the user details provided above\n",
    "\"\"\"\n",
    "TRUSTCALL_PROMPT = \"\"\"\n",
    "    Create or update the memory (JSON doc) to incorporate information from the following conversation:\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create helper functions to store and retrieve profiles\n",
    "\n",
    "# Extraction instruction\n",
    "\n",
    "\n",
    "def get_wellness_profile(store: BaseStore, user_id: str)-> ProfileSchema:\n",
    "    \"\"\"Retrieve a user's wellness profile.\n",
    "    returns a  of user profiles\"\"\"\n",
    "\n",
    "    user_profile =  store.get(('memory',user_id), 'user_profile')\n",
    "    return user_profile\n",
    "\n",
    "    \n",
    "\n",
    "def store_wellness_profile(store: BaseStore, user_id: str, user_profile: ProfileSchema):\n",
    "    store.put(('memory', user_id), 'user_profile', user_profile)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def update_profile_node(state: MemoryState, store: BaseStore,):\n",
    "    \"\"\"Reflect on the chat history and save a memory to the store.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = state[\"user_id\"]\n",
    "\n",
    "    user_profile=get_wellness_profile(store, user_id).value\n",
    "\n",
    "    # Retrieve existing memory from the store\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_profile\")\n",
    "    print(f\"existing_memory{existing_memory}\")\n",
    "\n",
    "    print(f\"existing_memory.value.model_dump(): {existing_memory.value.model_dump()}\")\n",
    "    # Get the profile as the value from the list, and convert it to a JSON doc\n",
    "\n",
    "    # Invoke the extractor\n",
    "    patch = trustcall_extractor.invoke(\n",
    "        {\n",
    "            \"messages\": [SystemMessage(content=TRUSTCALL_PROMPT)]+state[\"messages\"], \n",
    "            \"existing\":  {'ProfileSchema': existing_memory.value.model_dump()} \n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"patch{patch}\")\n",
    "    # Get the updated profile as a JSON object\n",
    "    updated_profile = existing_memory.value.model_copy(\n",
    "        update= {key: value for key, value in patch['responses'][0].model_dump().items() if value}\n",
    "    )\n",
    "\n",
    "    # Save the updated profile\n",
    "    key = \"user_profile\"\n",
    "    store.put(namespace, key, updated_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1339418570.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[476]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mexisting_profile{'ProfileSchema': ProfileSchema(name='dave', age='25', gender='male', occupation='fireman', conditions=['burns', 'lower back issues'], fitness_level='very fit', goals=['back pain relief', 'drink less'], sports=['boxing', 'weightlifting'], available_times=['varies depending on shift pattern'])}\u001b[39m\n                    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "existing_profile{'ProfileSchema': ProfileSchema(name='dave', age='25', gender='male', occupation='fireman', conditions=['burns', 'lower back issues'], fitness_level='very fit', goals=['back pain relief', 'drink less'], sports=['boxing', 'weightlifting'], available_times=['varies depending on shift pattern'])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the conversation\n",
    "\n",
    "\n",
    "# updated_conversation = [HumanMessage(content=\"Hi, I'm dave.\"), \n",
    "#                         AIMessage(content=\"Nice to meet you, dave.\"), \n",
    "#                         HumanMessage(content=\"I really like biking around San Francisco.\"),\n",
    "#                         AIMessage(content=\"San Francisco is a great city! Where do you go after biking?\"),\n",
    "#                         HumanMessage(content=\"I really like to go to a bakery after biking.\"),]\n",
    "\n",
    "# # Update the instruction\n",
    "# system_msg = f\"\"\"Update the memory (JSON doc) to incorporate new information from the following conversation and any existing fields presented as an array or list of items should also be incorporated\"\"\"\n",
    "\n",
    "# # Invoke the extractor with the updated instruction and existing profile with the corresponding tool name (UserProfile)\n",
    "# patch = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=system_msg)]+updated_conversation}, \n",
    "#                                     {\"existing\": {\"ProfileSchema\": user_profile_dave.model_dump()}})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create two different user profiles\n",
    "user_profile_dave = ProfileSchema(\n",
    "    name=\"dave\", \n",
    "    age=\"25\", \n",
    "    gender=\"male\", \n",
    "    occupation=\"fireman\",\n",
    "    conditions = [\"burns\", \"lower back issues\"],\n",
    "    fitness_level = \"very fit\",\n",
    "    medications = [\"sleeping pills\"],\n",
    "    goals= [\"back pain relief\", \"drink less\"],\n",
    "    sports = [\"boxing\", \"weightlifting\"],\n",
    "    available_times = [\"varies depending on shift pattern\"]\n",
    ")\n",
    "    \n",
    "\n",
    "user_profile_fiona = ProfileSchema(\n",
    "    name = \"fiona\", \n",
    "    age = \"75\", \n",
    "    gender = \"female\", \n",
    "    occupation = \"retired\",\n",
    "    conditions = [\"arthritis\", \"angina\", \"high cholesterol\"],\n",
    "    fitness_level = \"very fit\",\n",
    "    medications = [\"heart pills\", \"satins\"] ,\n",
    "    goals = [\"keep active\", \"meet new people\"],\n",
    "    sports = [\"walking\", \"dancing\"],\n",
    "    available_times = [\"weekdays\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Build a personalized agent that uses profiles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph = builder.compile(checkpointer=MemorySaver())\n",
    "# config = {\"configurable\": {\"thread_id\": \"1234\"}}\n",
    "# res = graph.invoke({\"messages\": [(\"user\", \"Hi there!\")]}, config)\n",
    "# res[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we are doing is calling this after the agent loop is finished before we go to end.\n",
    "#\n",
    "# def write_memories(messages_history):\n",
    "#     trustcall_extractor = create_extractor(\n",
    "#     llm,\n",
    "#     tools=[ProfileSchema],\n",
    "#     tool_choice=\"ProfileSchema\"\n",
    "# )\n",
    "\n",
    "\n",
    "#     # Update the instruction\n",
    "#     system_msg = f\"\"\"Update the memory (JSON doc) to incorporate new information from the following conversation and any existing fields presented as an array or list of items should also be incorporated\"\"\"\n",
    "\n",
    "#     # Invoke the extractor with the updated instruction and existing profile with the corresponding tool name (UserProfile)\n",
    "#     patch = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=system_msg)]+messages_history}, \n",
    "#                                     {\"existing\": {\"ProfileSchema\": user_profile_dave.model_dump()}})  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty fields to update the dict\n",
    "# user_profile_dave2 = user_profile_dave.model_copy(update= \n",
    "# {key: value for key, value in patch['responses'][0].model_dump().items() if value})\n",
    "\n",
    "# user_profile_dave2 = user_profile_dave.model_copy(update=patch['responses'][0].model_dump(exclude_unset=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wellness_assistant_node(state: MemoryState, store: BaseStore):\n",
    "    \"\"\"personalised wellness assistant dispensing advice with reference to user profile and preferences\"\"\"\n",
    "\n",
    "    \"\"\"Get User_profile  from the store  to personalise  response.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = state[\"user_id\"]\n",
    "    print(f\"user_id:{user_id}\")\n",
    "\n",
    "    # Retrieve memory from the store\n",
    "    user_profile = get_wellness_profile(store, user_id)\n",
    "    print(user_profile)\n",
    "    prompt = SYSTEM_PROMT + USER_PROMPT.format(\n",
    "    user_profile=user_profile.value.model_dump_json(), \n",
    "    )\n",
    "\n",
    "    # Respond using memory as well as the chat history\n",
    "    response = llm.invoke([SystemMessage(content=prompt)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALMAAAFNCAIAAADEmcl5AAAQAElEQVR4nOydB3wUxdvHZ6+n95AOBAgtgVBCk6YEkSYSEJAmTUCaNEFEpKkoiPgqf4EIUgQFpAoqIKEpIFUCoRqSEBJSSCXlLpe73fe52yS7JHdHCHewyT1f+OSzNzM7uzvz22eemd2dkTAMQxCkAhKCIIZAZSCGQWUghkFlIIZBZSCGQWUghqlmyjj/Z2bKXaWqQKvVkGK1rr8tElE0zYjFlFZLKIohFGFoIhFTNEMYLUQzlC4UAhlKBP+IVsvALoSBAEKJdIkpkYihaYrS/aS1pRlKRFoNzSbQZ86w4bojiiG7ku2KwKE1Wi5KLIHsiVRG3HwVjdo4+NSxJdUEqlqMZxxcn5wcq9KoGYmUSBUiqVxXybRaF8WICEUTkYSAVqAiiU4FhALB0yUigCrXyUWrjy3dANkQEEOJMii9TKDOCdGW/BSXZqhLIGb30icjumSUTmr6o8N+1GOnCmdCa7ifYimhGbpISWvUBM4fVOLsKX2pn2vtRg5E2AhdGb+sSky7r7ZxENVpbNf1TXexWEyqM1dOZt44m5+TUSxTUD3HevvWFa4JEa4yYs7mnNqdYeso6TOmlrufDalZ7F+XfP+W0t1fMmRmHSJIBKqMA5FJSbGqTgPcg9s6k5rLDwvvQhMzfll9IjyEqIx/T2RePJLzzmf1iBVwYP391Hj1O58K7mIFp4zdq+9nparf+cQqZMFyeEtK/PWCiV8Iy3KIiJA4sTs1I9m6ZAH0GOkd0Mj2h4/jiZAQljJiTuePXVSbWB+9RvuA+d6/LokIBgEp44eFcf4NbSRyKx2WHbM48P5tVVFhMREGQlHGtTPZygK63wRfYsV4+Mq2rxSK2RCKMi4czvatJyfWzeBZAXnZWiIMhKKMwkd0v4l+xOqxcxTtW5tMBIAglHF4c6pUQahyTyAszN27d/v06UOeng8++GD//v3EMvgH2aYnqogAEIQy0u6rXDxl5Ply48YNUiWqvGNlCOvupFYJYoRJEMpQFmhrBVjKycjLy1uxYkW/fv06deo0YcKEffv2QeDatWsXL16cmpraunXrbdu2QciOHTumTJnStWvXHj16zJs3LympxBPcvn07hJw4caJNmzZffvklpH/w4MHSpUshJbEATh428Dz2v+hc8qIRhDLg2YF3XQWxDKCAq1evQmXv2rUrODh42bJl8HPixIkjR4708vK6ePHisGHDrly5Aupp3rw51D2kz8rK+uijj9jdZTJZQUEB7LtkyZJBgwadPn0aAhcsWABaIZZBIiGpcUXkRSOIwQNwMBzdLNWaXL58GUTQrl072J46dWp4eLizc/mndCEhITt37gwICJBIdAVSXFw8Y8aM3NxcJycn8H5UKtXbb78dFhYGUUVFFq8zkURcWECTF40glKF/bcZSL16EhoZu3bo1JyenZcuW7du3b9y4ccU0YrEYmo+VK1fGxMSAhWADwXKAMtjtpk2bkucFTTOMALquwui1MkxBrqXuxUWLFg0dOvTs2bMzZ87s3r37mjVrNBpNuTQnT56E2CZNmnz//fcXLlxYvXp1uQTQppDnBa2l5bbPtZtmEEHYDKmMSk1U1mtmkRfgHB0dx4wZM3r06Ojo6OPHj2/YsMHBwWH48OH8NHv37gXTMnnyZPYnOK3kxUFriWfAix/0E4QyZAoqxTI+F/gKhw4dgo6JQqEI1XP79u1bt25VTObt7V3289ixY+QFUazSgjKaCuB9JUG0Ju5+8uw0izxJAo8yMjJy7ty5YDAyMzN/++03kAXoA6LA38zIyIAuxr1794KCgv755x/op0BDw3ZigZSUlIoZyuVyT0/PssTE3Jw++JC8+JZEhyCU0el1j6JCi3jjdnZ20B1NT08fO3YsDEts2bJl+vTpERERENWxY0eQyOzZsw8fPjxp0qQOHTqAqwEuKgxyQMcVfI5p06aBvamYJ7RN4IvMmjVLqVQScxMbne9SSxBvQQvlna61c+8GNLTtNcabWDerZ8QOmunn6W+p0Z3KI5QnasEdHONjCoh1s29NssJOJARZEOF8o9axn8e1v3Ojtqd0G2LYbMAQpDHHENp7doSqItBltdAwNmAiZxOnBMPwtWrVMhiVdEfZe7wnEQYCekM48Vb+gcjUyV8ZflEWGnVjHp+JarCxsTEW9eyY6NyaOCVwfUQiA6Z6y6fxEDx8Xl0iDIT17vje75Jy0tWjFwUSK+PcoYx/j+cI6vVxYb0h3H+Sn0gs+umLBGJNZKYXXjgiLFkQYX6JdHB9csYD9aiPhWJXLcqNi1nHf86avFJwn6kJ9OvFHz9LKCrUjqvpH57s/vZeWmLxpBX49eLT8Mfm5LvRSt96iv6Ta+D7oZeOZ5w/lCORknc+EaIsiMBnSVAr1Vs/TyrMo928pe16utQNdiTVnz82P7h3s5DQpElbx84DhNJHrUg1mFkl/lb+qV0ZBdkaeKCgsBPbO4nsHCVSuUij5R4w6GbR0c9zUobudWPdtVHsdkkUpZ96pRz6QC4NeWyyFDawLBY6loyWMBQXwm6IRLqSZJjy4WIR0RTTyjxNfi5d+EhD00RmQwJD7MPf8iLCpnrMucNy7XR23LXC3IdFxVqGaCh1EXfmumogFF8a+hBGV4e6bZOXCUlpqFvdNDqQTD9Nj14suvovvx/EamldpjwF6BLBTrT+d9mxSpQhFlESBvRh6yj2qWvbeYAHqSZUJ2VYmqioKHi6tnz5coLg3H58TAxcWiFYEByoDD5YEByoDD5YEBzFxcVSqZQgelAZHGgz+GBBcKAy+GBBcKAy+GBBcKCfwUdY72e8WFAZfFAZHNia8MGC4EBl8MGC4EBl8MGC4EBl8MGC4EBl8MGC4EBl8MGC4EBl8MGC4MDxDD6oDA60GXywIDhQGXywIDhQGXywIDhQGXywIDjQA+WDyuBAm8EHC4LDzc2tuq8hbUZQGRw5OTlqtZogelAZHNCUWGKKz2oKKoMDlcEHlcGByuCDyuBAZfBBZXCgMvigMjhgmAsGuwiiB5XBgTaDDyqDA5XBB5XBgcrgg8rgQGXwQWVwoAfKB5XBgTaDDyqDA5XBB5XBgcrgg9/Cc6Ay+KAyONAD5YNzCJPevXuzS7NSpfON0zTt5+d34MABYsWgzSBDhgwBayFi5xvXA9vdu3cn1g0qQ6cMf39/fggYjEGDBhHrBpWhcy+GDh0ql8vLQtq0aePlJfRVJiwNKkNHRESEr68vu+3p6Tls2DBi9aAyShgxYoStrS1shIWF1alTh1g9T+6bJN4p+O9yXpHK0M6UbjEiLV0ulF1iqGRNIn5i/RJFlG5JGd4iNSL9ujP6/Rh2v3LLELG7lMun/L6PrXrz+O765ZLIY2cCJ8ZbK6l0oaQLF84rC5WhLVo4OjpWPJaBHctfd/klmB5Lxq6XZCDPxy7TYM4lRfp4+RiE7V2ZrlKxmLh4Str0cCcmeYIyNnwcW1RIpHJRcZGBZJRIv56Qtlyo7jJEcJmUiKYZfmKGLommILo0qmy7RDoVlfF4fZTmw8+TlCxixDsLTjQi3QH5l8kdUb/BL2uaMCLCrZVVoRZL1zvinT8vumQJJujl0CX58061JLL8vtw6XPoTNqy5EgXrsijL3GAaSt8GMDQxgVROtBrdXdumh0urbm7GkpkaHV/3Qay7r+TVkXUIUuNIiMn9a/9DW3tx47bOBhMYtRnfz4/1a6Do2L8GroiJlLH1k9hXhro3bGFAHIY90LMH06GNQFnUeDwDpGd+zTQYZVgZif+pFA74GLbmExjiXFRguNEwXP3FhTQx6cUgNQM7V5nGyDNEw8qAjihDUwSp8dBGu7jYZCCGMaYMNBjWjrHRcVz42drB1gQxjGFlwBAsOyCNWC2GlQEj8wyNDYpVY9jPQHOBGLYZJY8+ESvAWD2jB2rt4EgX8nQYVgYlonBEw8oxMtLFPNexrt17toe/2pbdfiMifMuP60nNYuGiObNmv0uqFYaVweCXa2alc+du3bv3Mp1m8ZIPfv9jP3kG9u7bueyLhcRMoJ/xPOj2So8nprl9+0ZYWHvyDEAOxHyYRxkRA1/t9/qbb498B7Zzc3OgRejaJXzhx5+zsQMHvTYg4q23hrx9/frVzVsib9267uTs0r5dp7dHjrezszOWJ9wBP25d//VXkQsXz0lIiAsMrP/mwGGv9ejLxhrLKi8/b+Omtef++Ts7J6thUJPw8J69e71hItwE+fn5v+zaev7C2YSEu26u7h06dBkz+l2FQlGFo0Brkp+ft/LLNbD9z7nTO3ZsuXX7uqure3Bw8/Hjprq5ub/crTVErfhy6Zq1qw7sP2Hi0FC2o0dNhEKGy7exsQlr3X7K5NmQw/SZ46OjL0OCI0d+Ox51kTwzhlsTsUQkEj/FeEbr1u1u3LzGbl/+90KtWl7XYq6wP5MfJGVmZkCCpOT7s+dMUhWpVn+7ceniL+Pi/psxc7yJWQmkUimU5jffLn9/1oJjRy906Ry+fMWStLRUiDKR1fLli29cvzp9+rxNP+xq3Dh41dfLQEMmwk2wZ+/2n37eNHjQiM8+/XrChPdOnPwTKoONqvJR7vx3a96H77VoEQYJpk2dc/funS+WL4LwQ7+fhr/vz14AsjB9aCgWEJZIJNq3N2rzxt1Qzps2r4NwuIXgiK++2tsssiBGR7p0o+NPoYyWLcK+Xb1C9/I/RUVHX+rapfu+/TtBE74+fteu/evs7NKgfsNNmyOlEilUpJOT7n3U2bMWvDWs79+nT4B1MZZtcXExGIMmTUJgu8erfeB2jI29DbI7evQPY1lFX708ZPDIsNbtIHz8O1O7dAl3ctSlMRZugkFvDu/SuVvt2nXZnzEx0ecvnJkwfpqJ3J54lJhrV+DWHz5sDFQtXEijhk3i4mOf6tCAr68/5KDbsncAm3Hnzk1iAYw/N3kaF7RVy7aFhYXx8XfB5oOKx4x6F6wllIJeGVdatWxDdPY/ulGjpmxdAl5e3j4+flev/WtCGQDswm44OOi+DgIrYjqrkJDQnb9sBWPbvFlLaLYbBjVm0xgLNwHcnRcunv38i4Wxd++wBsnFxdV0bk88SnBIqEqlmjd/eutWbdu37+zn698itPVTHRoI4mULxVJQkE+qCjQZxhoH8/gZHh6e/v61Y65HQ4MH+gBrefNWDEikR48+UGFwGxF9pd66fYNtUMvIzso0nTNlaPDWRFZz5yz69dddx44fhhqyt7Pv33/wyBHvSCQSY+EmDh35/be//74PjDncl3B/r9/wv7K+Q5WPEtSg0efLvjl1Kgoy/27NKrhnRr09AbyNyh/aWJlUDRr+a5/mDeEqABcJrgY0HGA2bG1tQ0JagDMFd09SUiJ4iJDA1c0dbinwnvh7PdGkG8REVo4OjmBphw0dDRb4r7+P/7h1g729AxhnY+HGDgE288DB3QMHDO3Tuz8bwporlmc5Sts2HeA/nPylS+d27/n5w/nT9+z+s/KHfm4YfT/jaR/Ct2zZHb4kvQAAEABJREFUZs2aVfZ2Ds2bt4KfIcGhiYkJ4BAEBNRxddV9IlcvsMGRP38DMwtNLLsL9Dj8/ALI02Msq9xHuVFRh3r17AdtOUgH/oNfAk6fsXAThwAXR6lUurt7sj/VavWZs6fY7Wc5ypUrl4rURaAMd3cPMKheXj7Qp0hNS/EoPZDpQz9PjH4L/7QGq0VoGFzh2bOngpvqbCOYDfA6wcdu1apkcHPgwGE0Ta/+biU0tPfv31sX+c2YcYMN+l9PxFhWErEEfPhFS+bCLZuVlQn9t/9ib4FGjYWbOIRMJgNN/3HoV/CjwfIt/3IJpM/Le1RQUPAsR4EGd9HiOQcO7snJyb5xMwbKByTiVctbLpdDi3zx4j//XrkIcjd2aNPFAp7pzZsx0Dck5sCwMsADfVqbYW9v37BhkwcpydBPYUOaNm3G/wmWdsP6HTYKmwnvDh85asCV6EvQSYN2lzw9xrKCIY0li1ZkZKRPfW/sgDd7bN+5ZeKE6X37RBgLN32UBfM/U8gVo0YPHD7yDWgrx42bAj/7Dwh/lJdb5aNAy9K7V//V//uy/4Du0NO2tbVb9VUk64gMGzoGKnXBx7OUKqWxQ6ekPjBxwn17R4AL8v6cycQcGP6udfPSBOi1DphemyA1mpR45eFNyVO/rl8xytizVnx73Nqx6u9N+r7e1VjU3LmLOr7UlVgxRmwGbRUvCEdG/mQsysXZlVg3RsZAdX9qvtnw9vIhVo7I6KgZPoW3bmijT0EM91r18+kSxJoxOp6BL3VZOUZsBoVfI1k7RjxQBsczrB1j73RRIpxd2LoxMhuThsHZmKwc7LUihkFlIIYxrAyZjZjRaAlS02ForURqOMqwn2ljR1QqVEbNJy1RRRnpahgOfnmQuzIfu601n7hr+e6+coNRhpXh5GbjVVe2bVlVXsVDqgtR2xOLCjQDp/kbjDW1vsk/hx5eOZ7rVdfWt4GNja2s/J5VGgxjSvc1nUZkYn0XipeRwVOiSOmCNoZiS3amjO1bsnxNxbPSLU3DGDx33Qnrl2Yxcsole1UosfK5PXFVG4OxNKMfszZ21IrhNJOeXHjvxiO4pDGL6xFjhzP9gATEcfOffFWhVluZFW4rKRbmWYfenzmDqmRuutqqRNVurkpnY+SMRRJKKmFcvKUDp5l6mxNX8uWIioo6fPjw8uXLCYLjGXw0Go3pr9asCiwIDlQGHywIDlQGHywIDlQGHywIDlQGHywIDlQGHywIDlQGHywIjuLiYqlUShA9qAwOVAYffNuTA1sTPlgQHKgMPlgQHKgMPlgQHOhn8EFlcKDN4IMFwYHK4IMFwYHK4IMFwYHK4IMFwYHK4IMFwYHK4IMFwYHK4IMFwYHK4IMFwYEjXXxQGRxoM/hgQXD4+fmhzSgDlcGRnJysVqsJogeVwQEGA1wNguhBZXCAk6HV4qwhJaAyOMRisYn1Y60NVAYH2AxURhmoDA5UBh9UBgd6oHxQGRxoM/igMjhQGXxQGRyoDD6oDA5UBh9UBgd6oHxQGRxoM/igMjhQGXxQGRyoDD74LTwH+hl8UBkcaDP44BzCJDw8PDs7G8qhbI1a2HZ3dz9y5AixYtBmkF69eoEmRPrVi1kgsF27dsS6QWWQESNGBAQE8EM8PT2HDRtGrBtUBvHw8OjevTt/ufNmzZo1bNiQWDeoDB1gNvz8/NhtBwcHNBgElcFib2/ft29fkX714uDg4ObNmxOrxyIjXQk387TFRjRHMbrVhZ4OoysRUZXbhVsCxsjRIf6l0AHnGybm5ub27Dzi7tWC0lx0KyERk3DrzhjKvNxJsokNZltuNaaSc66YZ8UQmvFvKJPZlF+06hkxc691+8r4rBQtXJXWyLiA0eqswjpHz2EXQ+n162JVNrHhXI3lUKUzEkt0C2YpbKh+k33cvWyImTCnMn5cFqcpYl6KqOVd254gz5dTex7EXysctbC2vZN5PrMzmzI2Lo6T2VCvT6hLkBfH5sWx4z8NMEvLYh4P9PrZrKJCGmXxwnH3k+9Y9YCYA/Mo48a5PIU9dnNePIHBtnnZ5nn0Y57qLFJSEqmYIC8a51o2DE3Mgnl6rcVFWqrK3jZiRhiKNtOXufjmDmIYVEbNgjGb5UZl1CjM2KKjMmoUtPk6iOZRhkiE7qcgoMw3om0ejdE0vjRY08DWpEZBMWYbPEBl1CjAzzCX7TaTnyGmzOoXI1XEjH6GeZRBaxkcAhUGAhvPEIsoBrsngsBsQ13m6ZtooW+ifQGdk9FjB339f5+TakJcXOzcD6Z279Fu208bd+/Z3q17Gzb8jYjwLT+uJ+aBMlc1WMWj8/4Duj9ISSYvmqhjh65e+3fxwuXdXnmtSePgEcPHEfMjMD9DyKSmpuTkZBMBUFCQ7+Xl06FDZ9j28vJu3DiYCJgXpoyevTu+PXL8kMEj2Z/LVyy5e/fOurVbYbvP612GvjX69u0bp/46ZmdnFxLS4sN5Sx3sHSAqISHu8y8W3kuMDw1tPfLxe+7s2b+OHT8MN+WjR7mNGwWPGDGuRWjrf69cnDlrIsQOG97vpZe6fLJkpUaj2fDDd/+c+zs9PTU4OLR/v0Ht2nU0fap3/rs1YeLwxYuWb94SCS2Cm5v7y11fnTxpJtE3EGPfGbLs06+//OoTZ2eX9ZE/QyA0DYePHMzISPf09Apt3mrG9HkikWjqe2NjYqIh9uVurceNnaxQ2Hy35quoP8+XO9b161fhKLduXXdydmnfrhMUEZQAqTRmHM8QYmsiFkt+2bWtT5+IY0cvLP98dWJiwrerVxD9yjRz50318Ki16YddE96Ztn3HlszMDHYXlUr16bKPioqKPpi7+LNPvw4IqDP/oxlZWZkgDqg2SLBt636QBWx88+3yXbt/6v/G4J+2HejSudvCxXNOnooyfT4Sse7+2bp1wydLvzr8x5nJk2bt//WX337fR/QTK8DfLVvXDx40YtbMj2B746a1+/bvfHfC9F2/HB47ZtKJk3/CtUD4t/+3od/rA+vUCTwedXHY0NEGD5SUfH/2nEmqItXqbzcuXfxlXNx/M2aOf6rP8xnzdVvNowxKRInMqrH69YLCWrejKKpJkxAo0BMn/gRZgAlJT0+DiqlVywuKeNrUOfn5eWx6hUKxPnL7rJnzQQrwf+KE6Uql8lrMlXLZgnTgbh761qjX+w5wcnTq1bMfNPlbfvy+MqfUqdMr3l4+Mpns5a7dw8LaR0Ud0l24/haFU31z4LDGjZrm5ef9vH0zOBAdO3YFI9e1SzhIcOu2DZWcluPo0T+kEiloApQNFzh71oL/Ym+fPnOSvAjMU58MzdBmesmMpX597rNSXx9/KNkHD5KSk++DAqCFZsPBqnt61ipLVlhYAKZl4KDXwFxDUwUhFd2LO3duqtXqsNbty0LA2kOLkPsolzyJBo+fUsK9uLKfQQ0asxv379+DU+U7EEFBjfPz8+HMSSW4fj26UaOmTk7O7E+4Uh8fP2gfSaUx47CSefwMSkxRZh0DlcsVZdsKG93XNeC+gQNhY2NrMFlaWup7M8a1bNFmwfzPwMzArQydw4rZsjYGmvxy4dlZmWBCiEnAM+BtK+B8yn7K5HJ2IytL17opeCfPnrBSWUgqAZzerds3QNnlzo1UGsp8QwfmUYZuMOPZhKF9/PVFfrmrlEqirxhHR6dyRQx2gt2A5hyMATgZNnoZGeuMuLl7wF9odHx9/fnh4CqSJ1HWchG9W8MXShl2drpPsJQqZbkzdHV1J5XA1c09JCR09KiJ/EAnR2dSeSiB9Vr105E8nTRkMjm/msEO82Ojoy+VbUNbK5FIoC69anlDlYDxDwysD+GxsXcyMh6yacCcODg4srIAjDmVfr4Bcv39Db4IG5KdncUwjK2tLXkSV6IvgffAbsfG3g6sW79imnr1gsRiMTQK4HOwITdvxoDD4eHhSSpBvcAGR/78rXmzlqJSrw36Yn5+AeRFYCY/Q/d6xtOpFWw+1B+0wbD949YN0Mfjxz7MSAeXXqvVQsfk4G97Xn75VajRDh26gAMI/UPQB2hiySfzHEubgMDABtBP+fXAbvDkz50/c/nyeWitoV8KUf4BdeAv+LA3bsaAAka9PQFczmvXroCNgROAvkAlR1EvXDwLOcPG36dPQGc4PLxnxTSODo7dw3tt3fbDmTOnHuU9OnLkt737dgwcOExUOf8cUtI0vfq7lXCBcKusi/xmzLjBcfGxpPIIrTWpAlMmz1658pO+/bqCPYAuH/QRoDrLYvv07g89++/WrILtli3Cpk55n+jnMoAeaWTkNzDgAS39+HemHY36g03f7ZUe9+7FQZWv+noZ9BTmzlkEfdqfft6Ul/do5owPX+vRFzqTwU2br/pqHYygwJ390/ZNcDgw/k2bNJs166PKnPDQIaM2bPjfB/OmQTVHRAzp3esNg8mg6wQJln76IWgU/EcYmHlryNukcoCwNqzfsX375gnvDodbArzR92cvCGrQiLwIzPNd68ZF8dCeDJheh5iDfv27DYh4a+QISwweVwV2OOv/Vn3frFkLImxS4lSHNydN/bo+eWbwzZ2ahdA8UKlUxFTnN3fA7fhw/nRjsZ9+8hWpJphx8MBMXy8W02b8enH/3ijyfIG+YmTkT8ZiYegThrRJdYBhKGG97VcDgOonNQF8Co9YGPMoQyIRMTh9Rs3CPMrQaGicJUEIUAS/N0EMwRAGPVDEsqCfgRgG/QzEMNiaIIZBZSCGMY8yZHIRfvAsBETQbTWTw2eebBR2Inh0QpAXTWaaUmKeacfNpIzmne2VeWaahxJ5BmL/feToap4pe82jjPqhLnaO4j3fxBHkxZGfr8xO1wyda57Z3825isWub+7npBc17+raKMyVIM+RnCzlPwcfpieoJy6rK5aZx2aYeeWbfWuSUhNUGg15bPZrA0vCVAiqEEBV+omy0ZSGlqIxla051sUpjTK6uI2JdZZMrBdlIkos0p2GjQM1emE9Yj4sspKvMleZX8AplyIla0GV/tT90F0nxQstrbKymqMYquQ1lNJkuqKmSwqIv6uIoejSF1Yey5LLlAvlK6Nc4n8vXzp79uykyVPK6acsf0pfQWVXwVRYJqsskCqRDVXxlXqq9CyY8uGUPi1TpgD+7tw5UOX3FIm0buZbCqkMi4xn2DjZ2DiR6kdMnop+6OFj5gXJqik40sWh0WgkEiyQErAgOFAZfLAgOFAZfLAgOFAZfLAgOIqLi9k5dBCCyuCDNoMPFgQHKoMPFgQHKoMPvr3JgX4GH7xFONBm8MGC4EBl8MGC4EBl8MGC4EA/gw8qgwNtBh8sCA5UBh8sCA5UBh8sCA70M/igMjjQZvDBguBAZfDBguDQarWojDKwIDjAz0BllIEFwYGtCR8sCA5/f3+ZDD8pKAGVwZGYmFjJBc+sAVQGBwxmoDLKQGVwgJPxVEtg1mxQGRyoDD6oDA5UBh9UBgcqgw8qgwOVwQeVwYHK4IPK4EBl8OOZhOsAAAdqSURBVEFlcKAy+KAyOFAZfFAZHDgGygeVwYE2gw8qgwOVwQeVwYHK4IPK4EA/gw/OksCByuBjkTmEqxd9+vTRarXQjhQUFMCGWCwGfTg6OkZFPe9FyAUF2gwSEhKSnp6enZ2tVqtBGezfli1bEusGlUFGjx7t5eXFD/H09Bw0aBCxblAZJCgoqF27dvyQ+vXrh4WFEesGlaFjzJgxvr6+7LaTk9PgwYOJ1YPK0AGy6Nq1K7vkbO3atTt16kSsHlRGCcOHD/f397ezsxsyZAhBql2v9UF8/vnD2dmpGpWug6lfYIgIFBG7xI2ISGTEwUUS2Ny+7avupPpQbZRxfFfancv5miKGklAKO5mNs8LWSSqWycVVs3qVX4uryjAERkmUBWplZpEyT1Ws1K0f5uojGfp+HVIdqAbKuHkh++SuLIYm9rXs/Jt6kGpLZnJO+n/ZWjXxrS/vP9mfCBuhK2PnqsSHSWpXf0fvhm6kRqBWqu+eSwaL8u7y+kTACFoZPyyK02iooJcCSI3j/vWM3OS8cZ/VVdiYZxFNsyNcZWz7IiEvV9uoUx1SQ1EVFsWefjB2SV0beyGKQ6DK2PDxXZqiGrSrTWo0GrXm1on7U1YJsVkR4njGvu+SiotqviwAiUziVsfxf7NiifAQnDLSk5RJsapGXWu+LFi8g9ykcvFPK+4RgSE4Zexfm2LvZv4Vi4VMUKeArAfFOVkqIiSEpYwb57LVSrpOSy9iZcgdZPtXpxIhISxlnPsjB8qICJUr147OXtA2vyCbmBv/Zm552cJ6OVlYyih4pPVq6EqsD4WdQiwlhzanEMEgoHfHzx/OhMfg9s7W5WSUIbOVJt0pJIJBQMq4d6NQLLGgDbtw+eDZC3tT0mK9a9UPDQnv1H4I+0LGjzs+hHGdls1f27FnSVFRYW3/kN49ptT2D2b3Onjo24vRv8tlti2a9fB0t+BorL2H7cPYXCIYBNSa5GUXSxSWUurl6MM79i7182n44cy9Pbu/e+rM9v2/r2KjRCLJvfvXLl35472Jmz77+KREKtu+Zwkbdeb87jPnd0X0fv+9CRvdXHz+PL6BWAwXH3siJASkDLWKgZ49sQznL+0PrN0iou8cB3vXBoGte3Qbf/rcL3n5WWwsmIrB/T9yc/UViyUtm/V4mHEPQiD877M7mzXt1iz4FVtbx7CWfeoHtiYWQ6bQud7pSULpuwpIGbSWocQWOR+apuMTrwY1aFsWAuJgGDo+4Qr709Ojjlxuy24rFA7wt1D5CJ4bZGTdr+VZt2wvP59GxKJQJC9HKD0UAfkZIqmIsszrNBqNWqstPnR0Lfznh+cVlNgMijKgSFVRAU1ryxQDyGSW9Y7B7bGxEUqNCEgZUhkpVmuJBZDJFOBCtgrt1azpK/xwaD5M7KWQ24lE4uJizrwXqS3bd4CHm7XqCGVRJgEpw9ZBnJdtEWUAPt5BSlVe/cBW7E+NpjgzO9nZqZaJXaDn4uLsnZB4rctLJSE3b58mFiP7QZ5IRMRioTyRF5Cf4REg1xRbqpXt1f3dmJsnz136Vedz3Luydef8dRsnQytjeq/mweHXbhyHoU/YPvbXlntJMcRiPHpYIJVTRDAISBmde7vSljIZpG7t0BnvbgGXc9EXr63bNFWpyh89bIVUKje9V3iX0W1b9dv3+0oYFAeD8XrP6URn8y3iDKlyi1x9BPRkQFhv7nw//67cQRHQ3OqeqAHXj8ZHTPHxrmtLhIGwnpsEhtgVZAvrYfTzITE6TSYXCUcWRGhz7nQb4nXn0t30+GzPui4GE4CvUDZAWQ5bG0cYhDAYBS1C39emETMBbsqGrbMMRkEvFzrA7KB7OV5q+2bP8InECHkZhW16uBAhIbj3QE/tTbt+Jq/xK3UNxhaplQVGHoIXFSnlcsPjDTKZrb2dMzEfWdkPyFMil9vZ2ToZjIq/nKpVFo37JJAICSG+IbxxUTyMhga28SVWAJT/9aMJU74S3EvCQnxDePSiuspH6oykR8QKuHk8oeXLTkR4CPRb+Mkr66feyMxKqeHigP4ION0d+grxk0xBf6O2emasi6+9b5Nq/C2rCW4eS+j4hnvIS0I0GET437Wum3dXJBI16FijPmC8F52Wl1bYrJNj5whPIlSqwbfwv3ydmJaotnGS1av+PmnSzYe5D/KlUmrEfH8be0GvGlw95s/ISlP9ui6lIEcrkonsnBVOPnZOHsJ6A8oEWrX2YULOo/RCtVIDz8yatHfsOkC4pqKM6jTnjrpA/dvmtIdJanURoxtLogiMKjH8Z3AUOwcPO9DEEHbESRcG6anS2VQYUm6blE60QpVM4kOxG5QuP30kw1BUST66jZK99JlyM7RQ+hDdYfVhIrGubHXQRCKlbJ0lLbo6hnQQ1nCWCarrHMIp9wpT41SFeVrt49NB6+VQdkWlM+uUVCVFymr1sQRUqaTYLEolVaYwwtu3RBnkMTWVbOrVVHoOIhEts5O4eEoahArUxzQNzi6NGAbXKkAMg8pADIPKQAyDykAMg8pADIPKQAzz/wAAAP//QNHZYwAAAAZJREFUAwBeCfN/bUr2owAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the graph\n",
    "state = MemoryState()\n",
    "store = InMemoryStore()\n",
    "# instatiate wellness profiles\n",
    "store_wellness_profile(store, user_id=\"dave\", user_profile=user_profile_dave)\n",
    "\n",
    "# Checkpointer for short-term (within-thread) memory\n",
    "st_memory = MemorySaver()\n",
    "trustcall_extractor = create_extractor(\n",
    "    llm,\n",
    "    tools=[ProfileSchema],\n",
    "    tool_choice=\"ProfileSchema\"\n",
    ")\n",
    "\n",
    "\n",
    "builder = StateGraph(MemoryState)\n",
    "builder.add_node(\"wellness_assistant\", wellness_assistant_node)\n",
    "builder.add_node(\"update_profile\", update_profile_node)\n",
    "builder.add_edge(\"wellness_assistant\", \"update_profile\")\n",
    "builder.add_edge(START, \"wellness_assistant\")\n",
    "builder.add_edge(\"update_profile\", END)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compile the graph with the checkpointer fir and store\n",
    "assistant = builder.compile(checkpointer=st_memory, store=store)\n",
    "\n",
    "# View\n",
    "display(Image(assistant.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id:dave\n",
      "Item(namespace=['memory', 'dave'], key='user_profile', value=ProfileSchema(name='dave', age='25', gender='male', occupation='fireman', conditions=['burns', 'lower back issues'], fitness_level='very fit', goals=['back pain relief', 'drink less'], sports=['boxing', 'weightlifting', 'biking'], available_times=['varies depending on shift pattern']), created_at='2026-01-31T19:06:32.067157+00:00', updated_at='2026-01-31T19:06:32.067158+00:00')\n",
      "existing_memoryItem(namespace=['memory', 'dave'], key='user_profile', value=ProfileSchema(name='dave', age='25', gender='male', occupation='fireman', conditions=['burns', 'lower back issues'], fitness_level='very fit', goals=['back pain relief', 'drink less'], sports=['boxing', 'weightlifting', 'biking'], available_times=['varies depending on shift pattern']), created_at='2026-01-31T19:06:32.067157+00:00', updated_at='2026-01-31T19:06:32.067158+00:00')\n",
      "existing_memory.value.model_dump(): {'name': 'dave', 'age': '25', 'gender': 'male', 'occupation': 'fireman', 'conditions': ['burns', 'lower back issues'], 'fitness_level': 'very fit', 'goals': ['back pain relief', 'drink less'], 'sports': ['boxing', 'weightlifting', 'biking'], 'available_times': ['varies depending on shift pattern']}\n",
      "patch{'messages': [AIMessage(content='', additional_kwargs={'updated_docs': {'call_toLwrcKMprSmUHKXh9gxuZLu': 'ProfileSchema'}}, response_metadata={}, id='9c793a23-3505-448a-bca8-0fd1c1617d2d', tool_calls=[{'name': 'ProfileSchema', 'args': {'name': 'dave', 'age': '25', 'gender': 'male', 'occupation': 'fireman', 'conditions': ['burns', 'lower back issues'], 'fitness_level': 'very fit', 'goals': ['back pain relief', 'drink less'], 'sports': ['boxing', 'weightlifting', 'biking'], 'available_times': ['evenings after shifts']}, 'id': 'call_toLwrcKMprSmUHKXh9gxuZLu', 'type': 'tool_call'}], invalid_tool_calls=[])], 'responses': [ProfileSchema(name='dave', age='25', gender='male', occupation='fireman', conditions=['burns', 'lower back issues'], fitness_level='very fit', goals=['back pain relief', 'drink less'], sports=['boxing', 'weightlifting', 'biking'], available_times=['evenings after shifts'])], 'response_metadata': [{'id': 'call_toLwrcKMprSmUHKXh9gxuZLu', 'json_doc_id': 'ProfileSchema'}], 'attempts': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"Hi, I'm dave.\", additional_kwargs={}, response_metadata={}, id='6e8db0d3-9203-4691-b8d8-2c8c4041e6bf'),\n",
       "  AIMessage(content='Nice to meet you, dave.', additional_kwargs={}, response_metadata={}, id='ac864090-9672-4c42-9b31-4178793a0a8a', tool_calls=[], invalid_tool_calls=[]),\n",
       "  HumanMessage(content='I really like biking around San Francisco.', additional_kwargs={}, response_metadata={}, id='783ec2ab-1e19-4747-a40c-ee369b6f0b2f'),\n",
       "  AIMessage(content='San Francisco is a great city! Where do you go after biking?', additional_kwargs={}, response_metadata={}, id='0bd8d439-2e3e-4367-b538-894bd2838d8d', tool_calls=[], invalid_tool_calls=[]),\n",
       "  HumanMessage(content='I really like to go to a bakery after biking.', additional_kwargs={}, response_metadata={}, id='775e0658-0989-48f3-8f75-e0979f42c3fc'),\n",
       "  AIMessage(content=\"That sounds like a nice way to reward yourself after a bike ride! Since you're looking to drink less and manage your back pain, you might want to consider healthier options at the bakery. Here are a few suggestions:\\n\\n1. **Hydration**: Instead of sugary drinks, opt for water or herbal tea to stay hydrated after your ride.\\n\\n2. **Healthy Snacks**: Look for whole grain options or pastries made with less sugar. You might also consider a protein-rich snack, like a nut-based bar, to help with recovery.\\n\\n3. **Stretching**: After biking, take a few minutes to stretch your lower back and legs. This can help alleviate some of the tension and discomfort you might feel.\\n\\n4. **Balance**: Enjoy your treat, but try to balance it with healthier meals throughout the day to support your fitness goals.\\n\\nLet me know if you‚Äôd like more specific recommendations or tips!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 183, 'prompt_tokens': 244, 'total_tokens': 427, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1590f93f9d', 'id': 'chatcmpl-D4AaXd4XjMshzKRjOO19xc0zlQKe6', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c1573-0293-7dd3-94b1-8fc29eb2871f-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 244, 'output_tokens': 183, 'total_tokens': 427, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content=\"Hi, I'm dave.\", additional_kwargs={}, response_metadata={}, id='962ff62e-cfb8-4fb1-9f24-de8451e387ab'),\n",
       "  AIMessage(content='Nice to meet you, dave.', additional_kwargs={}, response_metadata={}, id='0e36e794-5a25-4e18-b02e-4ba1fc520276', tool_calls=[], invalid_tool_calls=[]),\n",
       "  HumanMessage(content='I really like biking.', additional_kwargs={}, response_metadata={}, id='71a4d457-6e7c-49e4-a8ad-161afb817598'),\n",
       "  AIMessage(content='When do you do that?', additional_kwargs={}, response_metadata={}, id='7eb89b7d-a6cc-4c79-8556-b8f876ffdefe', tool_calls=[], invalid_tool_calls=[]),\n",
       "  HumanMessage(content='After my shifts. Usually in the evening', additional_kwargs={}, response_metadata={}, id='ec48d0bc-7b4b-4f6a-be73-dc28601e1ae1'),\n",
       "  AIMessage(content=\"That sounds like a great way to unwind after a shift! Biking in the evening can be refreshing. Since you enjoy biking, here are a few tips to enhance your experience while also considering your goals:\\n\\n1. **Post-Ride Stretching**: After biking, take some time to stretch your lower back and legs. This can help alleviate any tension and support your back pain relief goal.\\n\\n2. **Hydration**: Make sure to drink plenty of water after your ride to stay hydrated, especially if you're biking in the evening.\\n\\n3. **Nutrition**: Consider packing a healthy snack to enjoy after your ride. Something like a banana or a protein bar can help with recovery.\\n\\n4. **Mindfulness**: Use your biking time to practice mindfulness. Focus on your surroundings, the rhythm of your pedaling, and your breathing. This can help improve your mental well-being.\\n\\n5. **Explore New Routes**: If you have the time, try exploring new biking routes in San Francisco. It can keep things exciting and help you discover new places.\\n\\nLet me know if you‚Äôd like more specific advice or have any other questions!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 229, 'prompt_tokens': 487, 'total_tokens': 716, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1590f93f9d', 'id': 'chatcmpl-D4AcRcUwmRUswADL0FFlN8OaNwk9l', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c1574-d05f-7132-9728-5b0b06770386-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 487, 'output_tokens': 229, 'total_tokens': 716, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
       " 'user_id': 'dave'}"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Test with different users - they should get different advice\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"thread1\"}}\n",
    "updated_conversation = [HumanMessage(content=\"Hi, I'm dave.\"), \n",
    "                        AIMessage(content=\"Nice to meet you, dave.\"), \n",
    "                        HumanMessage(content=\"I really like biking.\"),\n",
    "                        AIMessage(content=\"When do you do that?\"),\n",
    "                        HumanMessage(content=\"After my shifts. Usually in the evening\"),]\n",
    "\n",
    "assistant.invoke(\n",
    "    {\n",
    "        \"messages\": updated_conversation, \n",
    "        \"user_id\": \"dave\"\n",
    "    }, config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProfileSchema(name='dave', age='25', gender='male', occupation='fireman', conditions=['burns', 'lower back issues'], fitness_level='very fit', goals=['back pain relief', 'drink less'], sports=['boxing', 'weightlifting', 'biking'], available_times=['evenings after shifts'])"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_wellness_profile(store, 'dave').value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ü§ù Breakout Room #2\n",
    "## Advanced Memory & Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Semantic Memory (Embeddings + Search)\n",
    "\n",
    "**Semantic memory** stores facts and retrieves them based on *meaning* rather than exact matches. This is like how you might remember \"that restaurant with the great pasta\" even if you can't remember its exact name.\n",
    "\n",
    "In LangGraph, semantic memory uses:\n",
    "- **Store with embeddings**: Converts text to vectors for similarity search\n",
    "- **`store.search()`**: Finds relevant memories by semantic similarity\n",
    "\n",
    "### How It Works\n",
    "\n",
    "```\n",
    "User asks: \"What helps with headaches?\"\n",
    "         ‚Üì\n",
    "Query embedded ‚Üí [0.2, 0.8, 0.1, ...]\n",
    "         ‚Üì\n",
    "Compare with stored wellness facts:\n",
    "  - \"Hydration can relieve headaches\" ‚Üí 0.92 similarity ‚úì\n",
    "  - \"Exercise improves sleep\" ‚Üí 0.35 similarity\n",
    "         ‚Üì\n",
    "Return: \"Hydration can relieve headaches\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic memory store created with embedding support\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Create embeddings model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Create a store with semantic search enabled\n",
    "semantic_store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": embeddings,\n",
    "        \"dims\": 1536,  # Dimension of text-embedding-3-small\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Semantic memory store created with embedding support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 8 wellness facts in semantic memory\n"
     ]
    }
   ],
   "source": [
    "# Store various wellness facts as semantic memories\n",
    "namespace = (\"wellness\", \"facts\")\n",
    "\n",
    "wellness_facts = [\n",
    "    (\"fact_1\", {\"text\": \"Drinking water can help relieve headaches caused by dehydration\"}),\n",
    "    (\"fact_2\", {\"text\": \"Regular exercise improves sleep quality and helps you fall asleep faster\"}),\n",
    "    (\"fact_3\", {\"text\": \"Deep breathing exercises can reduce stress and anxiety within minutes\"}),\n",
    "    (\"fact_4\", {\"text\": \"Eating protein at breakfast helps maintain steady energy levels throughout the day\"}),\n",
    "    (\"fact_5\", {\"text\": \"Blue light from screens can disrupt your circadian rhythm and sleep\"}),\n",
    "    (\"fact_6\", {\"text\": \"Walking for 30 minutes daily can improve cardiovascular health\"}),\n",
    "    (\"fact_7\", {\"text\": \"Magnesium-rich foods like nuts and leafy greens can help with muscle cramps\"}),\n",
    "    (\"fact_8\", {\"text\": \"A consistent sleep schedule, even on weekends, improves overall sleep quality\"}),\n",
    "]\n",
    "\n",
    "for key, value in wellness_facts:\n",
    "    semantic_store.put(namespace, key, value)\n",
    "\n",
    "print(f\"Stored {len(wellness_facts)} wellness facts in semantic memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: My head hurts, what should I do?\n",
      "   Drinking water can help relieve headaches caused by dehydration (score: 0.327)\n",
      "   Magnesium-rich foods like nuts and leafy greens can help with muscle cramps (score: 0.173)\n",
      "\n",
      "Query: How can I get better rest at night?\n",
      "   Regular exercise improves sleep quality and helps you fall asleep faster (score: 0.463)\n",
      "   A consistent sleep schedule, even on weekends, improves overall sleep quality (score: 0.426)\n",
      "\n",
      "Query: I'm feeling stressed and anxious\n",
      "   Deep breathing exercises can reduce stress and anxiety within minutes (score: 0.415)\n",
      "   Drinking water can help relieve headaches caused by dehydration (score: 0.224)\n",
      "\n",
      "Query: What should I eat in the morning?\n",
      "   Eating protein at breakfast helps maintain steady energy levels throughout the day (score: 0.467)\n",
      "   Walking for 30 minutes daily can improve cardiovascular health (score: 0.249)\n"
     ]
    }
   ],
   "source": [
    "# Search semantically - notice we don't need exact matches!\n",
    "\n",
    "queries = [\n",
    "    \"My head hurts, what should I do?\",\n",
    "    \"How can I get better rest at night?\",\n",
    "    \"I'm feeling stressed and anxious\",\n",
    "    \"What should I eat in the morning?\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    results = semantic_store.search(namespace, query=query, limit=2)\n",
    "    for r in results:\n",
    "        print(f\"   {r.value['text']} (score: {r.score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Building Semantic Wellness Knowledge Base\n",
    "\n",
    "Let's load the HealthWellnessGuide.txt and create a semantic knowledge base that our agent can search.\n",
    "\n",
    "This is similar to RAG from Session 4, but now using LangGraph's Store API instead of a separate vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and split into 45 chunks\n",
      "\n",
      "Sample chunk:\n",
      "The Personal Wellness Guide\n",
      "A Comprehensive Resource for Health and Well-being\n",
      "\n",
      "PART 1: EXERCISE AND MOVEMENT\n",
      "\n",
      "Chapter 1: Understanding Exercise Basics\n",
      "\n",
      "Exercise is one of the most important things yo...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load and chunk the wellness document\n",
    "loader = TextLoader(\"data/HealthWellnessGuide.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Loaded and split into {len(chunks)} chunks\")\n",
    "print(f\"\\nSample chunk:\\n{chunks[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 45 chunks in semantic knowledge base\n"
     ]
    }
   ],
   "source": [
    "# Store chunks in semantic memory\n",
    "knowledge_namespace = (\"wellness\", \"knowledge\")\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    semantic_store.put(\n",
    "        knowledge_namespace,\n",
    "        f\"chunk_{i}\",\n",
    "        {\"text\": chunk.page_content, \"source\": \"HealthWellnessGuide.txt\"}\n",
    "    )\n",
    "\n",
    "print(f\"Stored {len(chunks)} chunks in semantic knowledge base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic wellness chatbot ready\n"
     ]
    }
   ],
   "source": [
    "# Build a semantic search wellness chatbot\n",
    "\n",
    "class SemanticState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_id: str\n",
    "\n",
    "\n",
    "def semantic_wellness_chatbot(state: SemanticState, config: RunnableConfig, *, store: BaseStore):\n",
    "    \"\"\"A wellness chatbot that retrieves relevant facts using semantic search.\"\"\"\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "    \n",
    "    # Search for relevant knowledge\n",
    "    knowledge_results = store.search(\n",
    "        (\"wellness\", \"knowledge\"),\n",
    "        query=user_message,\n",
    "        limit=3\n",
    "    )\n",
    "    \n",
    "    # Build context from retrieved knowledge\n",
    "    if knowledge_results:\n",
    "        knowledge_text = \"\\n\\n\".join([f\"- {r.value['text']}\" for r in knowledge_results])\n",
    "        system_msg = f\"\"\"You are a Personal Wellness Assistant with access to a wellness knowledge base.\n",
    "\n",
    "Relevant information from your knowledge base:\n",
    "{knowledge_text}\n",
    "\n",
    "Use this information to answer the user's question. If the information doesn't directly answer their question, use your general knowledge but mention what you found.\"\"\"\n",
    "    else:\n",
    "        system_msg = \"You are a Personal Wellness Assistant. Answer wellness questions helpfully.\"\n",
    "    \n",
    "    messages = [SystemMessage(content=system_msg)] + state[\"messages\"]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Build and compile\n",
    "builder3 = StateGraph(SemanticState)\n",
    "builder3.add_node(\"chatbot\", semantic_wellness_chatbot)\n",
    "builder3.add_edge(START, \"chatbot\")\n",
    "builder3.add_edge(\"chatbot\", END)\n",
    "\n",
    "semantic_graph = builder3.compile(\n",
    "    checkpointer=MemorySaver(),\n",
    "    store=semantic_store\n",
    ")\n",
    "\n",
    "print(\"Semantic wellness chatbot ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: What exercises can help with lower back pain?\n",
      "Assistant: There are several effective exercises that can help alleviate lower back pain. Here are some recommended ones:\n",
      "\n",
      "1. **Cat-Cow Stretch**: Start on your hands and knees. Alternate between arching your back up (like a cat) and letting it sag down (like a cow). Perform 10-15 repetitions.\n",
      "\n",
      "2. **Bird Dog**: From a hands-and-knees position, extend your opposite arm and leg while keeping your core engaged. Hold for 5 seconds, then switch sides. Aim for 10 repetitions per side.\n",
      "\n",
      "3. **Partial Crunches**: L...\n",
      "\n",
      "User: How can I improve my sleep quality?\n",
      "Assistant: Improving your sleep quality can be achieved through several essential sleep hygiene practices and creating an optimal sleep environment. Here are some tips:\n",
      "\n",
      "### Sleep Hygiene Practices:\n",
      "1. **Maintain a Consistent Sleep Schedule**: Go to bed and wake up at the same time every day, even on weekends.\n",
      "2. **Create a Relaxing Bedtime Routine**: Engage in calming activities before bed, such as reading, gentle stretching, or taking a warm bath.\n",
      "3. **Limit Screen Exposure**: Avoid screens (phones, tabl...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[126]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      4\u001b[39m questions = [\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWhat exercises can help with lower back pain?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mHow can I improve my sleep quality?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWhat should I eat for better gut health?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m ]\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     response = \u001b[43msemantic_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest_user\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mUser: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAssistant: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse[\u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m].content[:\u001b[32m500\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/langgraph/pregel/main.py:3071\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3068\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3069\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3071\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3085\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3086\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/langgraph/pregel/main.py:2646\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2644\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2645\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2646\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2656\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36msemantic_wellness_chatbot\u001b[39m\u001b[34m(state, config, store)\u001b[39m\n\u001b[32m     29\u001b[39m     system_msg = \u001b[33m\"\u001b[39m\u001b[33mYou are a Personal Wellness Assistant. Answer wellness questions helpfully.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m messages = [SystemMessage(content=system_msg)] + state[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [response]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1233\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1237\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1381\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1374\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1375\u001b[39m             response,\n\u001b[32m   1376\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1377\u001b[39m             metadata=generation_info,\n\u001b[32m   1378\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1379\u001b[39m         )\n\u001b[32m   1380\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1381\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1382\u001b[39m         response = raw_response.parse()\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/openai/_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/openai/_base_client.py:1294\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1285\u001b[39m     warnings.warn(\n\u001b[32m   1286\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing raw bytes as `body` is deprecated and will be removed in a future version. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1287\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease pass raw bytes via the `content` parameter instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1288\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m   1289\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m   1290\u001b[39m     )\n\u001b[32m   1291\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1292\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, content=content, files=to_httpx_files(files), **options\n\u001b[32m   1293\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/openai/_base_client.py:1002\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1000\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   1008\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/AIE9/06_Agent_Memory/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.12-macos-aarch64-none/lib/python3.11/ssl.py:1295\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1292\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1293\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1294\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.12-macos-aarch64-none/lib/python3.11/ssl.py:1168\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Test semantic retrieval\n",
    "config = {\"configurable\": {\"thread_id\": \"semantic_thread_1\"}}\n",
    "\n",
    "questions = [\n",
    "    \"What exercises can help with lower back pain?\",\n",
    "    \"How can I improve my sleep quality?\",\n",
    "    \"What should I eat for better gut health?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    response = semantic_graph.invoke(\n",
    "        {\"messages\": [HumanMessage(content=q)], \"user_id\": \"test_user\"},\n",
    "        config\n",
    "    )\n",
    "    print(f\"\\nUser: {q}\")\n",
    "    print(f\"Assistant: {response['messages'][-1].content[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Episodic Memory (Few-Shot Learning)\n",
    "\n",
    "**Episodic memory** stores past experiences and uses them as examples for future tasks. In humans, this is like remembering the first time you successfully gave someone good advice and using that experience to guide future interactions.\n",
    "\n",
    "For AI agents, episodic memory is often implemented as **few-shot examples** - showing the model examples of correct behavior so it can learn patterns.\n",
    "\n",
    "### The CoALA Framework Perspective\n",
    "\n",
    "> \"Facts can be written to semantic memory, whereas **experiences** can be written to episodic memory.\"\n",
    "\n",
    "Key insight: Sometimes it's easier to **show** than **tell**. LLMs learn well from examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 3 episodic memories (past successful interactions)\n"
     ]
    }
   ],
   "source": [
    "# Create episodic memory with past successful interactions\n",
    "episodic_namespace = (\"agent\", \"episodes\")\n",
    "\n",
    "successful_episodes = [\n",
    "    {\n",
    "        \"situation\": \"User asked about starting exercise with joint pain\",\n",
    "        \"input\": \"I want to start exercising but my knees hurt. What should I do?\",\n",
    "        \"output\": \"\"\"For knee-friendly exercise, I'd recommend starting with low-impact activities:\n",
    "\n",
    "1. **Swimming or water aerobics** - Water supports your joints while providing great cardio\n",
    "2. **Cycling** - Either stationary or regular bikes are gentle on knees\n",
    "3. **Elliptical machine** - Provides cardio without impact\n",
    "\n",
    "Start slowly with 15-20 minutes and gradually increase. If you experience pain, stop and consult a healthcare provider. Would you like specific exercises for any of these?\"\"\",\n",
    "        \"feedback\": \"User appreciated the specific alternatives and safety advice\"\n",
    "    },\n",
    "    {\n",
    "        \"situation\": \"User struggling with sleep and stress\",\n",
    "        \"input\": \"I can't fall asleep because I'm always thinking about work. Help!\",\n",
    "        \"output\": \"\"\"Racing thoughts at bedtime is common, especially with work stress. Here's a proven approach:\n",
    "\n",
    "**Before bed:**\n",
    "- Write down tomorrow's to-do list (gets thoughts out of your head)\n",
    "- Set a \"worry time\" earlier in the evening - not at bedtime\n",
    "\n",
    "**At bedtime:**\n",
    "- Try the 4-7-8 breathing technique: Inhale 4 counts, hold 7, exhale 8\n",
    "- Use the \"cognitive shuffle\" - think of random, unrelated objects (apple, guitar, mountain...)\n",
    "\n",
    "Which of these would you like to try first?\"\"\",\n",
    "        \"feedback\": \"User found the structured approach and specific techniques helpful\"\n",
    "    },\n",
    "    {\n",
    "        \"situation\": \"User asking about healthy eating on a budget\",\n",
    "        \"input\": \"How can I eat healthy when I don't have much money?\",\n",
    "        \"output\": \"\"\"Eating healthy on a budget is absolutely possible! Here are practical strategies:\n",
    "\n",
    "**Budget-friendly nutritious foods:**\n",
    "- Eggs, beans, lentils (cheap protein)\n",
    "- Frozen vegetables (same nutrition as fresh, longer lasting)\n",
    "- Oats, rice, whole grain bread\n",
    "- Canned fish (tuna, salmon)\n",
    "\n",
    "**Money-saving tips:**\n",
    "- Buy in bulk when possible\n",
    "- Plan meals around sales\n",
    "- Cook in batches and freeze portions\n",
    "\n",
    "What's your typical weekly food budget? I can help you create a specific meal plan.\"\"\",\n",
    "        \"feedback\": \"User valued the practical, actionable advice without judgment\"\n",
    "    },\n",
    "]\n",
    "\n",
    "for i, episode in enumerate(successful_episodes):\n",
    "    semantic_store.put(\n",
    "        episodic_namespace,\n",
    "        f\"episode_{i}\",\n",
    "        {\n",
    "            \"text\": episode[\"situation\"],  # Used for semantic search\n",
    "            **episode\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f\"Stored {len(successful_episodes)} episodic memories (past successful interactions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodic memory chatbot ready\n"
     ]
    }
   ],
   "source": [
    "class EpisodicState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "def episodic_wellness_chatbot(state: EpisodicState, config: RunnableConfig, *, store: BaseStore):\n",
    "    \"\"\"A chatbot that learns from past successful interactions.\"\"\"\n",
    "    user_question = state[\"messages\"][-1].content\n",
    "    \n",
    "    # Search for similar past experiences\n",
    "    similar_episodes = store.search(\n",
    "        (\"agent\", \"episodes\"),\n",
    "        query=user_question,\n",
    "        limit=1\n",
    "    )\n",
    "    \n",
    "    # Build few-shot examples from past episodes\n",
    "    if similar_episodes:\n",
    "        episode = similar_episodes[0].value\n",
    "        few_shot_example = f\"\"\"Here's an example of a similar wellness question I handled well:\n",
    "\n",
    "User asked: {episode['input']}\n",
    "\n",
    "My response was:\n",
    "{episode['output']}\n",
    "\n",
    "The user feedback was: {episode['feedback']}\n",
    "\n",
    "Use this as inspiration for the style, structure, and tone of your response, but tailor it to the current question.\"\"\"\n",
    "        \n",
    "        system_msg = f\"\"\"You are a Personal Wellness Assistant. Learn from your past successes:\n",
    "\n",
    "{few_shot_example}\"\"\"\n",
    "    else:\n",
    "        system_msg = \"You are a Personal Wellness Assistant. Be helpful, specific, and supportive.\"\n",
    "    \n",
    "    messages = [SystemMessage(content=system_msg)] + state[\"messages\"]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Build the episodic memory graph\n",
    "builder4 = StateGraph(EpisodicState)\n",
    "builder4.add_node(\"chatbot\", episodic_wellness_chatbot)\n",
    "builder4.add_edge(START, \"chatbot\")\n",
    "builder4.add_edge(\"chatbot\", END)\n",
    "\n",
    "episodic_graph = builder4.compile(\n",
    "    checkpointer=MemorySaver(),\n",
    "    store=semantic_store\n",
    ")\n",
    "\n",
    "print(\"Episodic memory chatbot ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: I want to exercise more but I have a bad hip. What can I do?\n",
      "\n",
      "Assistant: It's great that you want to exercise more! For a bad hip, focusing on low-impact activities can help you stay active while minimizing discomfort. Here are some options to consider:\n",
      "\n",
      "1. **Swimming or water aerobics** - The buoyancy of water reduces stress on your hip joints while providing a full-body workout.\n",
      "2. **Cycling** - Using a stationary bike or cycling outdoors can be gentle on your hips and still give you a good cardiovascular workout.\n",
      "3. **Walking** - Start with short, flat walks and gradually increase your distance. Consider using supportive shoes and walking on softer surfaces like grass or tracks.\n",
      "4. **Yoga or Pilates** - These practices can improve flexibility and strength without putting too much strain on your hips. Look for classes that focus on gentle movements.\n",
      "5. **Resistance training** - Focus on upper body and core exercises that don‚Äôt involve your hips too much. You can use resistance bands or light weights.\n",
      "\n",
      "Start with 15-20 minutes of activity and listen to your body. If you feel any pain, it's important to stop and consult a healthcare provider. Would you like specific exercises or routines for any of these options?\n",
      "\n",
      "Notice: The response structure mirrors the successful knee pain episode!\n"
     ]
    }
   ],
   "source": [
    "# Test episodic memory - similar question to stored episode\n",
    "config = {\"configurable\": {\"thread_id\": \"episodic_thread_1\"}}\n",
    "\n",
    "response = episodic_graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"I want to exercise more but I have a bad hip. What can I do?\")]},\n",
    "    config\n",
    ")\n",
    "\n",
    "print(\"User: I want to exercise more but I have a bad hip. What can I do?\")\n",
    "print(f\"\\nAssistant: {response['messages'][-1].content}\")\n",
    "print(\"\\nNotice: The response structure mirrors the successful knee pain episode!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Procedural Memory (Self-Improving Agent)\n",
    "\n",
    "**Procedural memory** stores the rules and instructions that guide behavior. In humans, this is like knowing *how* to give good advice - it's internalized knowledge about performing tasks.\n",
    "\n",
    "For AI agents, procedural memory often means **self-modifying prompts**. The agent can:\n",
    "1. Store its current instructions in the memory store\n",
    "2. Reflect on feedback from interactions\n",
    "3. Update its own instructions to improve\n",
    "\n",
    "### The Reflection Pattern\n",
    "\n",
    "```\n",
    "User feedback: \"Your advice is too long and complicated\"\n",
    "         ‚Üì\n",
    "Agent reflects on current instructions\n",
    "         ‚Üì\n",
    "Agent updates instructions: \"Keep advice concise and actionable\"\n",
    "         ‚Üì\n",
    "Future responses use updated instructions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized procedural memory with base instructions\n",
      "\n",
      "Current Instructions (v1):\n",
      "You are a Personal Wellness Assistant.\n",
      "\n",
      "Guidelines:\n",
      "- Be supportive and non-judgmental\n",
      "- Provide evidence-based wellness information\n",
      "- Ask clarifying questions when needed\n",
      "- Encourage healthy habits without being preachy\n"
     ]
    }
   ],
   "source": [
    "# Initialize procedural memory with base instructions\n",
    "procedural_namespace = (\"agent\", \"instructions\")\n",
    "\n",
    "initial_instructions = \"\"\"You are a Personal Wellness Assistant.\n",
    "\n",
    "Guidelines:\n",
    "- Be supportive and non-judgmental\n",
    "- Provide evidence-based wellness information\n",
    "- Ask clarifying questions when needed\n",
    "- Encourage healthy habits without being preachy\"\"\"\n",
    "\n",
    "semantic_store.put(\n",
    "    procedural_namespace,\n",
    "    \"wellness_assistant\",\n",
    "    {\"instructions\": initial_instructions, \"version\": 1}\n",
    ")\n",
    "\n",
    "print(\"Initialized procedural memory with base instructions\")\n",
    "print(f\"\\nCurrent Instructions (v1):\\n{initial_instructions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procedural memory graph ready (with self-improvement capability)\n"
     ]
    }
   ],
   "source": [
    "class ProceduralState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    feedback: str  # Optional feedback from user\n",
    "\n",
    "\n",
    "def get_instructions(store: BaseStore) -> tuple[str, int]:\n",
    "    \"\"\"Retrieve current instructions from procedural memory.\"\"\"\n",
    "    item = store.get((\"agent\", \"instructions\"), \"wellness_assistant\")\n",
    "    if item is None:\n",
    "        return \"You are a helpful wellness assistant.\", 0\n",
    "    return item.value[\"instructions\"], item.value[\"version\"]\n",
    "\n",
    "\n",
    "def procedural_assistant_node(state: ProceduralState, config: RunnableConfig, *, store: BaseStore):\n",
    "    \"\"\"Respond using current procedural instructions.\"\"\"\n",
    "    instructions, version = get_instructions(store)\n",
    "    \n",
    "    messages = [SystemMessage(content=instructions)] + state[\"messages\"]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def reflection_node(state: ProceduralState, config: RunnableConfig, *, store: BaseStore):\n",
    "    \"\"\"Reflect on feedback and update instructions if needed.\"\"\"\n",
    "    feedback = state.get(\"feedback\", \"\")\n",
    "    \n",
    "    if not feedback:\n",
    "        return {}  # No feedback, no update needed\n",
    "    \n",
    "    # Get current instructions\n",
    "    current_instructions, version = get_instructions(store)\n",
    "    \n",
    "    # Ask the LLM to reflect and improve instructions\n",
    "    reflection_prompt = f\"\"\"You are improving a wellness assistant's instructions based on user feedback.\n",
    "\n",
    "Current Instructions:\n",
    "{current_instructions}\n",
    "\n",
    "User Feedback:\n",
    "{feedback}\n",
    "\n",
    "Based on this feedback, provide improved instructions. Keep the same general format but incorporate the feedback.\n",
    "Only output the new instructions, nothing else.\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=reflection_prompt)])\n",
    "    new_instructions = response.content\n",
    "    \n",
    "    # Update procedural memory with new instructions\n",
    "    store.put(\n",
    "        (\"agent\", \"instructions\"),\n",
    "        \"wellness_assistant\",\n",
    "        {\"instructions\": new_instructions, \"version\": version + 1}\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nInstructions updated to version {version + 1}\")\n",
    "    return {}\n",
    "\n",
    "\n",
    "def should_reflect(state: ProceduralState) -> str:\n",
    "    \"\"\"Decide whether to reflect on feedback.\"\"\"\n",
    "    if state.get(\"feedback\"):\n",
    "        return \"reflect\"\n",
    "    return \"end\"\n",
    "\n",
    "\n",
    "# Build the procedural memory graph\n",
    "builder5 = StateGraph(ProceduralState)\n",
    "builder5.add_node(\"assistant\", procedural_assistant_node)\n",
    "builder5.add_node(\"reflect\", reflection_node)\n",
    "\n",
    "builder5.add_edge(START, \"assistant\")\n",
    "builder5.add_conditional_edges(\"assistant\", should_reflect, {\"reflect\": \"reflect\", \"end\": END})\n",
    "builder5.add_edge(\"reflect\", END)\n",
    "\n",
    "procedural_graph = builder5.compile(\n",
    "    checkpointer=MemorySaver(),\n",
    "    store=semantic_store\n",
    ")\n",
    "\n",
    "print(\"Procedural memory graph ready (with self-improvement capability)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: How can I reduce stress?\n",
      "\n",
      "Assistant (v1 instructions):\n",
      "Reducing stress is a great goal, and there are several effective strategies you can try. Here are some evidence-based methods:\n",
      "\n",
      "1. **Mindfulness and Meditation**: Practicing mindfulness or meditation can help you stay present and reduce anxiety. Even a few minutes a day can make a difference.\n",
      "\n",
      "2. **Physical Activity**: Regular exercise is a powerful stress reliever. It can boost your mood and improve your overall health. Find an activity you enjoy, whether it's walking, dancing, or yoga.\n",
      "\n",
      "3. **Deep Breathing**: Simple deep breathing exercises can help calm your mind and body. Try inhaling deeply through your nose, holding for a few seconds, and exhaling slowly through your mouth.\n",
      "\n",
      "4. **Connect with Others**: Talking to friends or family can provide support and help you feel less isolated. Sometimes just sharing your feelings can lighten the load.\n",
      "\n",
      "5. **Time Management**: Organizing your tasks and setting priorities can help you feel more in control and reduce feelings of overwhelm.\n",
      "\n",
      "6. **Healthy Eating**: A balanced diet can impact your mood and energy levels. Try to include plenty of fruits, vegetables, whole grains, and lean proteins in your meals.\n",
      "\n",
      "7. **Sleep Hygiene**: Ensure you‚Äôre getting enough quality sleep. Establish a calming bedtime routine and try to go to bed and wake up at the same time each day.\n",
      "\n",
      "8. **Limit Caffeine and Alcohol**: Both can increase feelings of anxiety and disrupt your sleep, so it might be helpful to monitor your intake.\n",
      "\n",
      "Is there a specific area of your life where you feel the most stress? That might help us find tailored strategies for you!\n"
     ]
    }
   ],
   "source": [
    "# Test with initial instructions\n",
    "config = {\"configurable\": {\"thread_id\": \"procedural_thread_1\"}}\n",
    "\n",
    "response = procedural_graph.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"How can I reduce stress?\")],\n",
    "        \"feedback\": \"\"  # No feedback yet\n",
    "    },\n",
    "    config\n",
    ")\n",
    "\n",
    "print(\"User: How can I reduce stress?\")\n",
    "print(f\"\\nAssistant (v1 instructions):\\n{response['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instructions updated to version 2\n"
     ]
    }
   ],
   "source": [
    "# Now provide feedback - the agent will update its own instructions!\n",
    "response = procedural_graph.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"How can I reduce stress?\")],\n",
    "        \"feedback\": \"Your responses are too long. Please be more concise and give me 3 actionable tips maximum.\"\n",
    "    },\n",
    "    {\"configurable\": {\"thread_id\": \"procedural_thread_2\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Instructions (v2):\n",
      "\n",
      "You are a Personal Wellness Assistant.\n",
      "\n",
      "Guidelines:\n",
      "- Be supportive and non-judgmental\n",
      "- Provide evidence-based wellness information\n",
      "- Ask clarifying questions when needed\n",
      "- Encourage healthy habits without being preachy\n",
      "- Keep responses concise, offering a maximum of 3 actionable tips.\n"
     ]
    }
   ],
   "source": [
    "# Check the updated instructions\n",
    "new_instructions, version = get_instructions(semantic_store)\n",
    "print(f\"Updated Instructions (v{version}):\\n\")\n",
    "print(new_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: How can I sleep better?\n",
      "\n",
      "Assistant (v2 instructions - after feedback):\n",
      "Improving your sleep can have a significant impact on your overall wellness. Here are three actionable tips to help you sleep better:\n",
      "\n",
      "1. **Establish a Consistent Sleep Schedule**: Try to go to bed and wake up at the same time every day, even on weekends. This helps regulate your body's internal clock.\n",
      "\n",
      "2. **Create a Relaxing Bedtime Routine**: Engage in calming activities before bed, such as reading, gentle stretching, or meditation. Avoid screens for at least 30 minutes before sleep, as blue light can interfere with melatonin production.\n",
      "\n",
      "3. **Optimize Your Sleep Environment**: Make your bedroom conducive to sleep by keeping it dark, cool, and quiet. Consider using blackout curtains, earplugs, or a white noise machine if needed.\n",
      "\n",
      "Would you like more specific strategies or information on any of these tips?\n",
      "\n",
      "Notice: The response should now be more concise based on the feedback!\n"
     ]
    }
   ],
   "source": [
    "# Test with updated instructions - should be more concise now!\n",
    "response = procedural_graph.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"How can I sleep better?\")],\n",
    "        \"feedback\": \"\"  # No feedback this time\n",
    "    },\n",
    "    {\"configurable\": {\"thread_id\": \"procedural_thread_3\"}}\n",
    ")\n",
    "\n",
    "print(f\"User: How can I sleep better?\")\n",
    "print(f\"\\nAssistant (v{version} instructions - after feedback):\")\n",
    "print(response['messages'][-1].content)\n",
    "print(\"\\nNotice: The response should now be more concise based on the feedback!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10: Unified Wellness Memory Agent\n",
    "\n",
    "Now let's combine **all 5 memory types** into a unified wellness agent:\n",
    "\n",
    "1. **Short-term**: Remembers current conversation (checkpointer)\n",
    "2. **Long-term**: Stores user profile across sessions (store + namespace)\n",
    "3. **Semantic**: Retrieves relevant wellness knowledge (store + embeddings)\n",
    "4. **Episodic**: Uses past successful interactions as examples (store + search)\n",
    "5. **Procedural**: Adapts behavior based on feedback (store + reflection)\n",
    "\n",
    "### Memory Retrieval Flow\n",
    "\n",
    "```\n",
    "User Query: \"What exercises can help my back pain?\"\n",
    "              ‚îÇ\n",
    "              ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  1. PROCEDURAL: Get current instructions         ‚îÇ\n",
    "‚îÇ  2. LONG-TERM: Load user profile (conditions)    ‚îÇ\n",
    "‚îÇ  3. SEMANTIC: Search wellness knowledge          ‚îÇ\n",
    "‚îÇ  4. EPISODIC: Find similar past interactions     ‚îÇ\n",
    "‚îÇ  5. SHORT-TERM: Include conversation history     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "              ‚îÇ\n",
    "              ‚ñº\n",
    "        Generate personalized, informed response\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified wellness assistant ready with all 5 memory types!\n"
     ]
    }
   ],
   "source": [
    "class UnifiedState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_id: str\n",
    "    feedback: str\n",
    "\n",
    "\n",
    "def unified_wellness_assistant(state: UnifiedState, config: RunnableConfig, *, store: BaseStore):\n",
    "    \"\"\"An assistant that uses all five memory types.\"\"\"\n",
    "    user_id = state[\"user_id\"]\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "    \n",
    "    # 1. PROCEDURAL: Get current instructions\n",
    "    instructions_item = store.get((\"agent\", \"instructions\"), \"wellness_assistant\")\n",
    "    base_instructions = instructions_item.value[\"instructions\"] if instructions_item else \"You are a helpful wellness assistant.\"\n",
    "    \n",
    "    # 2. LONG-TERM: Get user profile\n",
    "    profile_items = list(store.search((user_id, \"profile\")))\n",
    "    pref_items = list(store.search((user_id, \"preferences\")))\n",
    "    profile_text = \"\\n\".join([f\"- {p.key}: {p.value}\" for p in profile_items]) if profile_items else \"No profile stored.\"\n",
    "    \n",
    "    # 3. SEMANTIC: Search for relevant knowledge\n",
    "    relevant_knowledge = store.search((\"wellness\", \"knowledge\"), query=user_message, limit=2)\n",
    "    knowledge_text = \"\\n\".join([f\"- {r.value['text'][:200]}...\" for r in relevant_knowledge]) if relevant_knowledge else \"No specific knowledge found.\"\n",
    "    \n",
    "    # 4. EPISODIC: Find similar past interactions\n",
    "    similar_episodes = store.search((\"agent\", \"episodes\"), query=user_message, limit=1)\n",
    "    if similar_episodes:\n",
    "        ep = similar_episodes[0].value\n",
    "        episode_text = f\"Similar past interaction:\\nUser: {ep.get('input', 'N/A')}\\nResponse style: {ep.get('feedback', 'N/A')}\"\n",
    "    else:\n",
    "        episode_text = \"No similar past interactions found.\"\n",
    "    \n",
    "    # Build comprehensive system message\n",
    "    system_message = f\"\"\"{base_instructions}\n",
    "\n",
    "=== USER PROFILE ===\n",
    "{profile_text}\n",
    "\n",
    "=== RELEVANT WELLNESS KNOWLEDGE ===\n",
    "{knowledge_text}\n",
    "\n",
    "=== LEARNING FROM EXPERIENCE ===\n",
    "{episode_text}\n",
    "\n",
    "Use all of this context to provide the best possible personalized response.\"\"\"\n",
    "    \n",
    "    # 5. SHORT-TERM: Full conversation history is automatically managed by the checkpointer\n",
    "    # Use summarization for long conversations\n",
    "    trimmed_messages = summarize_conversation(state[\"messages\"], max_messages=6)\n",
    "    \n",
    "    messages = [SystemMessage(content=system_message)] + trimmed_messages\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def unified_feedback_node(state: UnifiedState, config: RunnableConfig, *, store: BaseStore):\n",
    "    \"\"\"Update procedural memory based on feedback.\"\"\"\n",
    "    feedback = state.get(\"feedback\", \"\")\n",
    "    if not feedback:\n",
    "        return {}\n",
    "    \n",
    "    item = store.get((\"agent\", \"instructions\"), \"wellness_assistant\")\n",
    "    if item is None:\n",
    "        return {}\n",
    "    \n",
    "    current = item.value\n",
    "    reflection_prompt = f\"\"\"Update these instructions based on feedback:\n",
    "\n",
    "Current: {current['instructions']}\n",
    "Feedback: {feedback}\n",
    "\n",
    "Output only the updated instructions.\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=reflection_prompt)])\n",
    "    store.put(\n",
    "        (\"agent\", \"instructions\"),\n",
    "        \"wellness_assistant\",\n",
    "        {\"instructions\": response.content, \"version\": current[\"version\"] + 1}\n",
    "    )\n",
    "    print(f\"Procedural memory updated to v{current['version'] + 1}\")\n",
    "    return {}\n",
    "\n",
    "\n",
    "def unified_route(state: UnifiedState) -> str:\n",
    "    return \"feedback\" if state.get(\"feedback\") else \"end\"\n",
    "\n",
    "\n",
    "# Build the unified graph\n",
    "unified_builder = StateGraph(UnifiedState)\n",
    "unified_builder.add_node(\"assistant\", unified_wellness_assistant)\n",
    "unified_builder.add_node(\"feedback\", unified_feedback_node)\n",
    "\n",
    "unified_builder.add_edge(START, \"assistant\")\n",
    "unified_builder.add_conditional_edges(\"assistant\", unified_route, {\"feedback\": \"feedback\", \"end\": END})\n",
    "unified_builder.add_edge(\"feedback\", END)\n",
    "\n",
    "# Compile with both checkpointer (short-term) and store (all other memory types)\n",
    "unified_graph = unified_builder.compile(\n",
    "    checkpointer=MemorySaver(),\n",
    "    store=semantic_store\n",
    ")\n",
    "\n",
    "print(\"Unified wellness assistant ready with all 5 memory types!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What exercises would you recommend for my back?\n",
      "\n",
      "Assistant: It's great that you're looking to support your back health! Here are three gentle exercises that can help relieve lower back pain:\n",
      "\n",
      "1. **Cat-Cow Stretch**: Start on your hands and knees. Alternate between arching your back up (like a cat) and letting it sag down (like a cow). Aim for 10-15 repetitions to help increase flexibility and relieve tension.\n",
      "\n",
      "2. **Bird-Dog**: From the same hands-and-knees position, extend one arm forward and the opposite leg back, keeping your back straight. Hold for a few seconds, then switch sides. This helps strengthen your core and lower back.\n",
      "\n",
      "3. **Child‚Äôs Pose**: Kneel on the floor, sit back on your heels, and stretch your arms forward on the ground. This pose gently stretches the back and can provide relief.\n",
      "\n",
      "Make sure to listen to your body and stop if you feel any pain. Have you tried any of these exercises before, or do you have any specific concerns about your back?\n",
      "\n",
      "============================================================\n",
      "Memory types used:\n",
      "  Long-term: Knows Sarah has a bad knee\n",
      "  Semantic: Retrieved back exercise info from knowledge base\n",
      "  Episodic: May use similar joint pain episode as reference\n",
      "  Procedural: Following current instructions\n",
      "  Short-term: Will remember this in follow-up questions\n"
     ]
    }
   ],
   "source": [
    "# Test the unified assistant\n",
    "config = {\"configurable\": {\"thread_id\": \"unified_thread_1\"}}\n",
    "\n",
    "# First interaction - should use semantic + long-term + episodic memory\n",
    "response = unified_graph.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"What exercises would you recommend for my back?\")],\n",
    "        \"user_id\": \"user_sarah\",  # Sarah has a bad knee in her profile!\n",
    "        \"feedback\": \"\"\n",
    "    },\n",
    "    config\n",
    ")\n",
    "\n",
    "print(\"User: What exercises would you recommend for my back?\")\n",
    "print(f\"\\nAssistant: {response['messages'][-1].content}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Memory types used:\")\n",
    "print(\"  Long-term: Knows Sarah has a bad knee\")\n",
    "print(\"  Semantic: Retrieved back exercise info from knowledge base\")\n",
    "print(\"  Episodic: May use similar joint pain episode as reference\")\n",
    "print(\"  Procedural: Following current instructions\")\n",
    "print(\"  Short-term: Will remember this in follow-up questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Can you show me how to do the first one?\n",
      "\n",
      "Assistant: Absolutely! Here‚Äôs how to do the Cat-Cow Stretch step-by-step:\n",
      "\n",
      "1. **Start Position**: Begin on your hands and knees on a comfortable surface, like a yoga mat. Your wrists should be directly under your shoulders, and your knees should be under your hips.\n",
      "\n",
      "2. **Cat Pose**: Inhale deeply. As you exhale, round your back towards the ceiling, tucking your chin to your chest and drawing your belly button in towards your spine. This is the \"cat\" position.\n",
      "\n",
      "3. **Cow Pose**: Inhale again. As you exhale, arch your back, letting your belly drop towards the floor while lifting your head and tailbone towards the ceiling. This is the \"cow\" position.\n",
      "\n",
      "4. **Repeat**: Continue to alternate between these two positions for 10-15 repetitions, coordinating your breath with your movements.\n",
      "\n",
      "Remember to move slowly and gently, focusing on your breath. How does that sound? Would you like tips on how to incorporate this into your routine?\n",
      "\n",
      "Notice: The agent remembers the context from the previous message!\n"
     ]
    }
   ],
   "source": [
    "# Follow-up question (tests short-term memory)\n",
    "response = unified_graph.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Can you show me how to do the first one?\")],\n",
    "        \"user_id\": \"user_sarah\",\n",
    "        \"feedback\": \"\"\n",
    "    },\n",
    "    config  # Same thread\n",
    ")\n",
    "\n",
    "print(\"User: Can you show me how to do the first one?\")\n",
    "print(f\"\\nAssistant: {response['messages'][-1].content}\")\n",
    "print(\"\\nNotice: The agent remembers the context from the previous message!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚ùì Question #3:\n",
    "\n",
    "How would you decide what constitutes a **\"successful\" wellness interaction** worth storing as an episode? What metadata should you store alongside the episode?\n",
    "\n",
    "Consider:\n",
    "- Explicit feedback (thumbs up) vs implicit signals\n",
    "- User engagement (did they ask follow-up questions?)\n",
    "- Objective outcomes vs subjective satisfaction\n",
    "- Privacy implications of storing interaction data\n",
    "\n",
    "##### Answer:\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùì Question #4:\n",
    "\n",
    "For a **production wellness assistant**, which memory types need persistent storage (PostgreSQL) vs in-memory? How would you handle memory across multiple agent instances (e.g., Exercise Agent, Nutrition Agent, Sleep Agent)?\n",
    "\n",
    "Consider:\n",
    "- Which memories are user-specific vs shared?\n",
    "- Consistency requirements across agents\n",
    "- Memory expiration and cleanup policies\n",
    "- Namespace strategy for multi-agent systems\n",
    "\n",
    "##### Answer:\n",
    "User specfic - personal details. Shared would be systemic patterns where we might need to adjust Procedural memory to update the prompt to make the responses better aligned with overall user preferences or tackle unforseen edge cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üèóÔ∏è Activity #2: Wellness Memory Dashboard\n",
    "\n",
    "Build a wellness tracking system that:\n",
    "1. Tracks wellness metrics over time (mood, energy, sleep quality)\n",
    "2. Uses semantic memory to find relevant advice\n",
    "3. Uses episodic memory to recall what worked before\n",
    "4. Uses procedural memory to adapt advice style\n",
    "5. Provides a synthesized \"wellness summary\"\n",
    "\n",
    "### Requirements:\n",
    "- Store at least 3 wellness metrics per user\n",
    "- Track metrics over multiple \"days\" (simulated)\n",
    "- Agent should reference historical data in responses\n",
    "- Generate a personalized wellness summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Step 1: Define wellness metrics schema and storage functions\n",
    "def log_wellness_metric(store, user_id: str, date: str, metric_type: str, value: float, notes: str = \"\"):\n",
    "    \"\"\"Log a wellness metric for a user.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_wellness_history(store, user_id: str, metric_type: str = None, days: int = 7) -> list:\n",
    "    \"\"\"Get wellness history for a user.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# Step 2: Create sample wellness data for a user (simulate a week)\n",
    "\n",
    "\n",
    "# Step 3: Build a wellness dashboard agent that:\n",
    "#   - Retrieves user's wellness history\n",
    "#   - Searches for relevant advice based on patterns\n",
    "#   - Uses episodic memory for what worked before\n",
    "#   - Generates a personalized summary\n",
    "\n",
    "\n",
    "# Step 4: Test the dashboard\n",
    "# Example: \"Give me a summary of my wellness this week\"\n",
    "# Example: \"I've been feeling tired lately. What might help?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this session, we explored the **5 memory types** from the CoALA framework:\n",
    "\n",
    "| Memory Type | LangGraph Component | Scope | Wellness Use Case |\n",
    "|-------------|---------------------|-------|-------------------|\n",
    "| **Short-term** | `MemorySaver` + `thread_id` | Within thread | Current consultation |\n",
    "| **Long-term** | `InMemoryStore` + namespaces | Across threads | User profile, goals |\n",
    "| **Semantic** | Store + embeddings + `search()` | Across threads | Knowledge retrieval |\n",
    "| **Episodic** | Store + few-shot examples | Across threads | Past successful interactions |\n",
    "| **Procedural** | Store + self-reflection | Across threads | Self-improving instructions |\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Memory transforms chatbots into assistants** - Persistence enables personalization\n",
    "2. **Different memory types serve different purposes** - Choose based on your use case\n",
    "3. **Context management is critical** - Trim and summarize to stay within limits\n",
    "4. **Episodic memory enables learning** - Show, don't just tell\n",
    "5. **Procedural memory enables adaptation** - Agents can improve themselves\n",
    "\n",
    "### Production Considerations:\n",
    "\n",
    "- Use `PostgresSaver` instead of `MemorySaver` for persistent checkpoints\n",
    "- Use `PostgresStore` instead of `InMemoryStore` for persistent long-term memory\n",
    "- Consider TTL (Time-to-Live) policies for automatic memory cleanup\n",
    "- Implement proper access controls for user data\n",
    "\n",
    "### Further Reading:\n",
    "\n",
    "- [LangGraph Memory Documentation](https://langchain-ai.github.io/langgraph/concepts/memory/)\n",
    "- [CoALA Paper](https://arxiv.org/abs/2309.02427) - Cognitive Architectures for Language Agents\n",
    "- [LangGraph Platform](https://docs.langchain.com/langgraph-platform/) - Managed infrastructure for production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
